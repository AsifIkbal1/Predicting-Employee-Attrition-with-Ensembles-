{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ CONTENTS ◽\n    </center>\n</h1>\n\n#### [**<span style=\"color:black\">1. Importing the Libraries</span>**](#one)\n    \n#### [**<span style=\"color:black\">2. Loading the Dataset</span>**](#two)\n\n#### [**<span style=\"color:black\">3. Problem statement</span>**](#three)\n\n#### [**<span style=\"color:black\">4. Exploratory Data Analysis (Phase 1)</span>**](#four)\n\n#### [**<span style=\"color:black\">5. Feature Engineering</span>**](#five)\n\n#### [**<span style=\"color:black\">6. Exploratory Data Analysis (Phase 2)</span>**](#six)\n\n#### [**<span style=\"color:black\">7. Encoding Categorical Features + Train-Test Split</span>**](#seven)\n\n#### [**<span style=\"color:black\">8. Simple Decision Tree Classifier</span>**](#eight)\n\n#### [**<span style=\"color:black\">9. Gradient Boosted Decision Trees using XGBoost</span>**](#nine)\n\n#### [**<span style=\"color:black\">10. Random Forest Classifier</span>**](#ten)\n\n#### [**<span style=\"color:black\">11. Comparison of Models</span>**](#eleven)\n\n#### [**<span style=\"color:black\">12. Insights obtained from EDA</span>**](#twelve)\n\n#### [**<span style=\"color:black\">13. Discussion on Trade Off and Possible Recommendations</span>**](#thirteen)\n\n\n<a id = \"one\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ IMPORTING LIBRARIES ◽\n    </center>\n</h1>\n\n","metadata":{}},{"cell_type":"code","source":"# importing required libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display\nfrom itertools import cycle\n\nimport category_encoders as ce\nimport datetime as dt\nimport warnings\n\nfrom sklearn import tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, StratifiedKFold\nfrom sklearn.model_selection import learning_curve, train_test_split\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\n\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\npd.set_option('display.max_columns',None)\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:43.151072Z","iopub.execute_input":"2022-05-25T14:23:43.15164Z","iopub.status.idle":"2022-05-25T14:23:44.153931Z","shell.execute_reply.started":"2022-05-25T14:23:43.151448Z","shell.execute_reply":"2022-05-25T14:23:44.152871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n<a id = \"two\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ LOADING THE DATASET ◽\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/predicting-employee-attrition/train_MpHjUjU.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.156201Z","iopub.execute_input":"2022-05-25T14:23:44.156554Z","iopub.status.idle":"2022-05-25T14:23:44.206919Z","shell.execute_reply.started":"2022-05-25T14:23:44.156508Z","shell.execute_reply":"2022-05-25T14:23:44.205941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"three\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ PROBLEM STATEMENT ◽\n    </center>\n</h1>","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.208365Z","iopub.execute_input":"2022-05-25T14:23:44.208735Z","iopub.status.idle":"2022-05-25T14:23:44.231334Z","shell.execute_reply.started":"2022-05-25T14:23:44.20869Z","shell.execute_reply":"2022-05-25T14:23:44.230294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(df.columns)\nprint(\"No. of columns : \", len(df.columns))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.233099Z","iopub.execute_input":"2022-05-25T14:23:44.233598Z","iopub.status.idle":"2022-05-25T14:23:44.247297Z","shell.execute_reply.started":"2022-05-25T14:23:44.23355Z","shell.execute_reply":"2022-05-25T14:23:44.246345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.249119Z","iopub.execute_input":"2022-05-25T14:23:44.249753Z","iopub.status.idle":"2022-05-25T14:23:44.262974Z","shell.execute_reply.started":"2022-05-25T14:23:44.249715Z","shell.execute_reply":"2022-05-25T14:23:44.261693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Data types of all the attributes","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.264349Z","iopub.execute_input":"2022-05-25T14:23:44.264993Z","iopub.status.idle":"2022-05-25T14:23:44.300495Z","shell.execute_reply.started":"2022-05-25T14:23:44.26493Z","shell.execute_reply":"2022-05-25T14:23:44.299492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[df.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.301713Z","iopub.execute_input":"2022-05-25T14:23:44.301972Z","iopub.status.idle":"2022-05-25T14:23:44.331528Z","shell.execute_reply.started":"2022-05-25T14:23:44.301941Z","shell.execute_reply":"2022-05-25T14:23:44.330649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong>So there are no duplicates in our data. </strong>","metadata":{}},{"cell_type":"code","source":"#converting categorical features to 'object' type\ndf['Emp_ID'] = df['Emp_ID'].astype(\"object\")\ndf['Education_Level'] = df['Education_Level'].astype(\"object\")\ndf['Gender'] = df['Gender'].astype('object')\ndf['Joining Designation'] = df['Joining Designation'].astype('object')\ndf['Designation'] = df['Designation'].astype('object')\ndf['Quarterly Rating'] = df['Quarterly Rating'].astype('object')\n\n#date-time features\ndf['MMM-YY'] = pd.to_datetime(df[\"MMM-YY\"]).astype('datetime64[ns]')\ndf['Dateofjoining'] = pd.to_datetime(df[\"Dateofjoining\"]).astype('datetime64[ns]')\ndf['LastWorkingDate'] = pd.to_datetime(df[\"LastWorkingDate\"]).astype('datetime64[ns]')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.33315Z","iopub.execute_input":"2022-05-25T14:23:44.333634Z","iopub.status.idle":"2022-05-25T14:23:44.367583Z","shell.execute_reply.started":"2022-05-25T14:23:44.333589Z","shell.execute_reply":"2022-05-25T14:23:44.366893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### \n#### 5. Statistical Summary","metadata":{}},{"cell_type":"code","source":"#numerical features\ndf.describe(include=[np.number]).T","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.3688Z","iopub.execute_input":"2022-05-25T14:23:44.369634Z","iopub.status.idle":"2022-05-25T14:23:44.404509Z","shell.execute_reply.started":"2022-05-25T14:23:44.369585Z","shell.execute_reply":"2022-05-25T14:23:44.403558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#categorical features\ndf.describe(include=['object']).T","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.40565Z","iopub.execute_input":"2022-05-25T14:23:44.405873Z","iopub.status.idle":"2022-05-25T14:23:44.465582Z","shell.execute_reply.started":"2022-05-25T14:23:44.405847Z","shell.execute_reply":"2022-05-25T14:23:44.464485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#date-time features\ndf.describe(exclude = ['object','float64', 'int64'])[0:6].T","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.466924Z","iopub.execute_input":"2022-05-25T14:23:44.467182Z","iopub.status.idle":"2022-05-25T14:23:44.496204Z","shell.execute_reply.started":"2022-05-25T14:23:44.467151Z","shell.execute_reply":"2022-05-25T14:23:44.495189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### \n#### 6. Missing Value Detection","metadata":{}},{"cell_type":"code","source":"percent = (df.isnull().sum()*100/df.isnull().count()).sort_values(ascending=False)\npercent_idx = percent.index\nabsolute = (df.isnull().sum())[percent_idx]\n\nmissing = pd.concat([percent, absolute], axis=1, keys = ['Percentage Missing', 'Total Missing'])\nmissing[missing[\"Total Missing\"]>0]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.497416Z","iopub.execute_input":"2022-05-25T14:23:44.497774Z","iopub.status.idle":"2022-05-25T14:23:44.563305Z","shell.execute_reply.started":"2022-05-25T14:23:44.49773Z","shell.execute_reply":"2022-05-25T14:23:44.56239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7. Unique Values","metadata":{}},{"cell_type":"markdown","source":"We will look at the number of unique categories of each categorical feature.","metadata":{}},{"cell_type":"code","source":"cat_cols = df.select_dtypes(include=['object']).columns.tolist()\ndf[cat_cols].nunique()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.564787Z","iopub.execute_input":"2022-05-25T14:23:44.565027Z","iopub.status.idle":"2022-05-25T14:23:44.589013Z","shell.execute_reply.started":"2022-05-25T14:23:44.564999Z","shell.execute_reply":"2022-05-25T14:23:44.588283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"five\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Feature Engineering ◽\n    </center>\n</h1>\nWe will need to perform certain aggregations so that we are left with one record per employee.","metadata":{}},{"cell_type":"code","source":"newdf = pd.DataFrame()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.592353Z","iopub.execute_input":"2022-05-25T14:23:44.592982Z","iopub.status.idle":"2022-05-25T14:23:44.59768Z","shell.execute_reply.started":"2022-05-25T14:23:44.592941Z","shell.execute_reply":"2022-05-25T14:23:44.596799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will now put the unique Employee IDs as a column and aggregate over the values of other columns as necessary.","metadata":{}},{"cell_type":"code","source":"newdf[\"E_ID\"] = df[\"Emp_ID\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.598905Z","iopub.execute_input":"2022-05-25T14:23:44.599174Z","iopub.status.idle":"2022-05-25T14:23:44.617642Z","shell.execute_reply.started":"2022-05-25T14:23:44.599137Z","shell.execute_reply":"2022-05-25T14:23:44.616654Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf[\"Age\"] = df.groupby('Emp_ID').agg({'Age':'max'})['Age'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.619177Z","iopub.execute_input":"2022-05-25T14:23:44.620275Z","iopub.status.idle":"2022-05-25T14:23:44.638477Z","shell.execute_reply.started":"2022-05-25T14:23:44.620221Z","shell.execute_reply":"2022-05-25T14:23:44.637658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf[\"Gender\"] = df.groupby('Emp_ID').agg({'Gender':'last'})['Gender'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.639801Z","iopub.execute_input":"2022-05-25T14:23:44.640434Z","iopub.status.idle":"2022-05-25T14:23:44.658052Z","shell.execute_reply.started":"2022-05-25T14:23:44.640386Z","shell.execute_reply":"2022-05-25T14:23:44.656817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Education Level","metadata":{}},{"cell_type":"code","source":"df.groupby(\"Emp_ID\")[\"Education_Level\"].nunique().sort_values().iloc[[0,-1]]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.659475Z","iopub.execute_input":"2022-05-25T14:23:44.659803Z","iopub.status.idle":"2022-05-25T14:23:44.682863Z","shell.execute_reply.started":"2022-05-25T14:23:44.659758Z","shell.execute_reply":"2022-05-25T14:23:44.681896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf[\"Education\"] = df.groupby('Emp_ID').agg({'Education_Level':'last'})['Education_Level'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.684033Z","iopub.execute_input":"2022-05-25T14:23:44.684268Z","iopub.status.idle":"2022-05-25T14:23:44.698505Z","shell.execute_reply.started":"2022-05-25T14:23:44.684241Z","shell.execute_reply":"2022-05-25T14:23:44.697851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. City","metadata":{}},{"cell_type":"code","source":"df.groupby(\"Emp_ID\")[\"City\"].nunique().sort_values().iloc[[0,-1]]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.699539Z","iopub.execute_input":"2022-05-25T14:23:44.700118Z","iopub.status.idle":"2022-05-25T14:23:44.724711Z","shell.execute_reply.started":"2022-05-25T14:23:44.700071Z","shell.execute_reply":"2022-05-25T14:23:44.724086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It appears that each employee is only associated to one unique city. So again, we can pick either the first or last.","metadata":{}},{"cell_type":"code","source":"newdf[\"City\"] = df.groupby('Emp_ID').agg({'City':'first'})['City'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.725984Z","iopub.execute_input":"2022-05-25T14:23:44.726197Z","iopub.status.idle":"2022-05-25T14:23:44.745048Z","shell.execute_reply.started":"2022-05-25T14:23:44.726171Z","shell.execute_reply":"2022-05-25T14:23:44.744309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6. Income","metadata":{}},{"cell_type":"code","source":"df.groupby(\"Emp_ID\")[\"Salary\"].nunique().sort_values().iloc[[0,-1]]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.74637Z","iopub.execute_input":"2022-05-25T14:23:44.747168Z","iopub.status.idle":"2022-05-25T14:23:44.76146Z","shell.execute_reply.started":"2022-05-25T14:23:44.747128Z","shell.execute_reply":"2022-05-25T14:23:44.760686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf['Salary'] = df.groupby('Emp_ID').agg({'Salary':'last'})['Salary'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.762958Z","iopub.execute_input":"2022-05-25T14:23:44.763383Z","iopub.status.idle":"2022-05-25T14:23:44.7787Z","shell.execute_reply.started":"2022-05-25T14:23:44.763342Z","shell.execute_reply":"2022-05-25T14:23:44.77792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 7. Raise (in Salary)","metadata":{}},{"cell_type":"code","source":"first_salary = df.groupby('Emp_ID').agg({'Salary':'first'})\nlast_salary = df.groupby('Emp_ID').agg({'Salary':'last'})\n\nc1 = first_salary > last_salary\nc2 = first_salary == last_salary\nc3 = first_salary < last_salary\n\nprint(\"Salary has decreased for\", c1[c1[\"Salary\"]==True].shape[0], \"employees.\")\nprint(\"Salary has remained unchanged for\", c2[c2[\"Salary\"]==True].shape[0], \"employees.\")\nprint(\"Salary has increased for\", c3[c3[\"Salary\"]==True].shape[0], \"employees.\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.779799Z","iopub.execute_input":"2022-05-25T14:23:44.78071Z","iopub.status.idle":"2022-05-25T14:23:44.80861Z","shell.execute_reply.started":"2022-05-25T14:23:44.780654Z","shell.execute_reply":"2022-05-25T14:23:44.80738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1 if income has increased, 0 otherwise.\nnewdf[\"Raise\"] = np.where(c3[\"Salary\"]==True, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.810298Z","iopub.execute_input":"2022-05-25T14:23:44.811036Z","iopub.status.idle":"2022-05-25T14:23:44.817387Z","shell.execute_reply.started":"2022-05-25T14:23:44.810985Z","shell.execute_reply":"2022-05-25T14:23:44.816155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 8. Joining Designation and Current Designation (Grade)","metadata":{}},{"cell_type":"code","source":"df.groupby(\"Emp_ID\")[\"Joining Designation\"].nunique().sort_values().iloc[[0,-1]]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.818611Z","iopub.execute_input":"2022-05-25T14:23:44.819006Z","iopub.status.idle":"2022-05-25T14:23:44.842418Z","shell.execute_reply.started":"2022-05-25T14:23:44.818969Z","shell.execute_reply":"2022-05-25T14:23:44.841371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(\"Emp_ID\")[\"Designation\"].nunique().sort_values().iloc[[0,-1]]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.843716Z","iopub.execute_input":"2022-05-25T14:23:44.843971Z","iopub.status.idle":"2022-05-25T14:23:44.86607Z","shell.execute_reply.started":"2022-05-25T14:23:44.843941Z","shell.execute_reply":"2022-05-25T14:23:44.865136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The joining designation of the employees has not changed as expected. The current designation has changed for some (most likely they were promoted).","metadata":{}},{"cell_type":"code","source":"newdf[\"Joining Designation\"] = df.groupby('Emp_ID').agg({'Joining Designation':'first'})['Joining Designation'].values\nnewdf[\"Current Designation\"] = df.groupby('Emp_ID').agg({'Designation':'last'})['Designation'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.867325Z","iopub.execute_input":"2022-05-25T14:23:44.868093Z","iopub.status.idle":"2022-05-25T14:23:44.896306Z","shell.execute_reply.started":"2022-05-25T14:23:44.868052Z","shell.execute_reply":"2022-05-25T14:23:44.895363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 9. Promotion (in Grade)","metadata":{}},{"cell_type":"code","source":"first_des = df.groupby('Emp_ID').agg({'Joining Designation':'first'}).rename(columns={'Joining Designation':'Grade'})\nlast_des = df.groupby('Emp_ID').agg({'Designation':'last'}).rename(columns={'Designation':'Grade'})\n\nc1 = first_des > last_des\nc2 = first_des == last_des\nc3 = first_des < last_des\n\nprint(c1[c1[\"Grade\"]==True].shape[0], \"employees were demoted.\")\nprint(c2[c2[\"Grade\"]==True].shape[0], \"employees have unchanged grade.\")\nprint(c3[c3[\"Grade\"]==True].shape[0], \"employees were promoted.\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.897973Z","iopub.execute_input":"2022-05-25T14:23:44.898693Z","iopub.status.idle":"2022-05-25T14:23:44.93395Z","shell.execute_reply.started":"2022-05-25T14:23:44.898645Z","shell.execute_reply":"2022-05-25T14:23:44.932896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1 if promoted, 0 otherwise.\nnewdf[\"Promoted\"] = np.where(c3[\"Grade\"]==True, 1, 0)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.93566Z","iopub.execute_input":"2022-05-25T14:23:44.936223Z","iopub.status.idle":"2022-05-25T14:23:44.943123Z","shell.execute_reply.started":"2022-05-25T14:23:44.936173Z","shell.execute_reply":"2022-05-25T14:23:44.94189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 10. Total Business Value","metadata":{}},{"cell_type":"markdown","source":"We take the total business value generated by the employee as sum of all such values.","metadata":{}},{"cell_type":"code","source":"newdf[\"Total Business Value\"] = df.groupby(\"Emp_ID\").agg({'Total Business Value':'sum'})['Total Business Value'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.944812Z","iopub.execute_input":"2022-05-25T14:23:44.945345Z","iopub.status.idle":"2022-05-25T14:23:44.966162Z","shell.execute_reply.started":"2022-05-25T14:23:44.945297Z","shell.execute_reply":"2022-05-25T14:23:44.965098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 11. Quarterly Rating","metadata":{}},{"cell_type":"markdown","source":"We take the latest quarterly rating of the employee.","metadata":{}},{"cell_type":"code","source":"newdf[\"Quarterly Rating\"] = df.groupby('Emp_ID').agg({'Quarterly Rating':'last'})['Quarterly Rating'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.968438Z","iopub.execute_input":"2022-05-25T14:23:44.969212Z","iopub.status.idle":"2022-05-25T14:23:44.987391Z","shell.execute_reply.started":"2022-05-25T14:23:44.969102Z","shell.execute_reply":"2022-05-25T14:23:44.986491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 12. Increase in Quarterly Rating","metadata":{}},{"cell_type":"code","source":"first_q = df.groupby('Emp_ID').agg({'Quarterly Rating':'first'})\nlast_q = df.groupby('Emp_ID').agg({'Quarterly Rating':'last'})\n\nc1 = first_q > last_q\nc2 = first_q == last_q\nc3 = first_q < last_q\n\nprint(c1[c1[\"Quarterly Rating\"]==True].shape[0], \"employees had a decrease in quarterly rating.\")\nprint(c2[c2[\"Quarterly Rating\"]==True].shape[0], \"employees had unchanged quarterly rating.\")\nprint(c3[c3[\"Quarterly Rating\"]==True].shape[0], \"employees had an increase in quarterly rating.\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:44.989393Z","iopub.execute_input":"2022-05-25T14:23:44.990085Z","iopub.status.idle":"2022-05-25T14:23:45.026464Z","shell.execute_reply.started":"2022-05-25T14:23:44.990034Z","shell.execute_reply":"2022-05-25T14:23:45.02554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we assign -1 for those who had reduced rating\n#we assign 0 for unchanged rating\n#we assign +1 for those who had increased rating\n\nnewdf[\"qr_increased\"] = np.where(c3[\"Quarterly Rating\"]==True, 1, (np.where(c2[\"Quarterly Rating\"]==True, 0, -1)))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.027735Z","iopub.execute_input":"2022-05-25T14:23:45.027988Z","iopub.status.idle":"2022-05-25T14:23:45.034144Z","shell.execute_reply.started":"2022-05-25T14:23:45.027961Z","shell.execute_reply":"2022-05-25T14:23:45.033311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 13. Experience","metadata":{}},{"cell_type":"code","source":"newdf[\"last_report\"] = df.groupby('Emp_ID').agg({'MMM-YY':'last'})['MMM-YY'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.035266Z","iopub.execute_input":"2022-05-25T14:23:45.035975Z","iopub.status.idle":"2022-05-25T14:23:45.056177Z","shell.execute_reply.started":"2022-05-25T14:23:45.035933Z","shell.execute_reply":"2022-05-25T14:23:45.05516Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf[\"Dateofjoining\"] = df.groupby('Emp_ID').agg({'Dateofjoining':'first'})['Dateofjoining'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.057677Z","iopub.execute_input":"2022-05-25T14:23:45.057966Z","iopub.status.idle":"2022-05-25T14:23:45.079628Z","shell.execute_reply.started":"2022-05-25T14:23:45.057922Z","shell.execute_reply":"2022-05-25T14:23:45.078613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf[\"Experience\"] = newdf[\"last_report\"] - newdf[\"Dateofjoining\"]\nnewdf[\"Experience\"] = newdf[\"Experience\"].astype(\"timedelta64[D]\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.081205Z","iopub.execute_input":"2022-05-25T14:23:45.081933Z","iopub.status.idle":"2022-05-25T14:23:45.096246Z","shell.execute_reply.started":"2022-05-25T14:23:45.081889Z","shell.execute_reply":"2022-05-25T14:23:45.095459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf.drop(columns=[\"last_report\",\"Dateofjoining\"], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.097604Z","iopub.execute_input":"2022-05-25T14:23:45.097909Z","iopub.status.idle":"2022-05-25T14:23:45.119948Z","shell.execute_reply.started":"2022-05-25T14:23:45.097874Z","shell.execute_reply":"2022-05-25T14:23:45.118897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 14. Target column","metadata":{}},{"cell_type":"markdown","source":"We need to check which employees have left/ attrited.","metadata":{}},{"cell_type":"code","source":"newdf[\"attrited\"] = df.groupby(\"Emp_ID\").agg({'LastWorkingDate':'last'})['LastWorkingDate'].values","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.121954Z","iopub.execute_input":"2022-05-25T14:23:45.122289Z","iopub.status.idle":"2022-05-25T14:23:45.141629Z","shell.execute_reply.started":"2022-05-25T14:23:45.122183Z","shell.execute_reply":"2022-05-25T14:23:45.140296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign 0 values if last working day does not eist, assign 1 otherwise.\nnewdf[\"attrited\"] = np.where(newdf[\"attrited\"].isnull(), 0, 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.143103Z","iopub.execute_input":"2022-05-25T14:23:45.143354Z","iopub.status.idle":"2022-05-25T14:23:45.156166Z","shell.execute_reply.started":"2022-05-25T14:23:45.143322Z","shell.execute_reply":"2022-05-25T14:23:45.155115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"six\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Exploratory Data Analysis (Phase 2) ◽\n    </center>\n</h1>\nWe need to repeat certain EDA steps since we now have a new dataset after performing the aggregations.","metadata":{}},{"cell_type":"code","source":"newdf.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.165742Z","iopub.execute_input":"2022-05-25T14:23:45.166276Z","iopub.status.idle":"2022-05-25T14:23:45.185063Z","shell.execute_reply.started":"2022-05-25T14:23:45.166222Z","shell.execute_reply":"2022-05-25T14:23:45.18391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.18669Z","iopub.execute_input":"2022-05-25T14:23:45.187228Z","iopub.status.idle":"2022-05-25T14:23:45.209867Z","shell.execute_reply.started":"2022-05-25T14:23:45.187179Z","shell.execute_reply":"2022-05-25T14:23:45.208878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1. Shape of New Data","metadata":{}},{"cell_type":"code","source":"newdf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.211367Z","iopub.execute_input":"2022-05-25T14:23:45.211911Z","iopub.status.idle":"2022-05-25T14:23:45.221233Z","shell.execute_reply.started":"2022-05-25T14:23:45.211862Z","shell.execute_reply":"2022-05-25T14:23:45.220221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong>So we have 15 columns (of which one is target column) and 2381 observations now.  </strong>\n\n","metadata":{}},{"cell_type":"markdown","source":"#### 2. Rechecking Missing Values","metadata":{}},{"cell_type":"code","source":"percent = (newdf.isnull().sum()*100/newdf.isnull().count()).sort_values(ascending=False)\npercent_idx = percent.index\nabsolute = (newdf.isnull().sum())[percent_idx]\n\nmissing = pd.concat([percent, absolute], axis=1, keys = ['Percentage Missing', 'Total Missing'])\nmissing[missing[\"Total Missing\"]>0]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.222602Z","iopub.execute_input":"2022-05-25T14:23:45.223426Z","iopub.status.idle":"2022-05-25T14:23:45.256152Z","shell.execute_reply.started":"2022-05-25T14:23:45.223384Z","shell.execute_reply":"2022-05-25T14:23:45.255205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong>So we see that there are no more missing values in our new data after the feature engineering steps.  </strong>","metadata":{}},{"cell_type":"markdown","source":"#### 3. Converting features of new data to their respective data type.\n\nWe will convert the data types of certain features.","metadata":{}},{"cell_type":"code","source":"newdf['E_ID'] = newdf['E_ID'].astype(\"object\")\nnewdf['Education'] = newdf['Education'].astype(\"object\")\nnewdf['Gender'] = newdf['Gender'].astype('object')\nnewdf['Joining Designation'] = newdf['Joining Designation'].astype('object')\nnewdf['Current Designation'] = newdf['Current Designation'].astype('object')\nnewdf['Quarterly Rating'] = newdf['Quarterly Rating'].astype('object')\nnewdf['Raise'] = newdf['Raise'].astype('object')\nnewdf['Promoted'] = newdf['Promoted'].astype('object')\nnewdf['qr_increased'] = newdf['qr_increased'].astype('object')\nnewdf['attrited'] = newdf['attrited'].astype('object')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.257644Z","iopub.execute_input":"2022-05-25T14:23:45.258439Z","iopub.status.idle":"2022-05-25T14:23:45.274417Z","shell.execute_reply.started":"2022-05-25T14:23:45.258401Z","shell.execute_reply":"2022-05-25T14:23:45.273681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Statistical Summary of New Data","metadata":{}},{"cell_type":"code","source":"newdf.describe(include=[np.number]).T","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.275899Z","iopub.execute_input":"2022-05-25T14:23:45.276564Z","iopub.status.idle":"2022-05-25T14:23:45.310159Z","shell.execute_reply.started":"2022-05-25T14:23:45.27652Z","shell.execute_reply":"2022-05-25T14:23:45.309115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf.describe(include=['object']).T","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.311816Z","iopub.execute_input":"2022-05-25T14:23:45.312161Z","iopub.status.idle":"2022-05-25T14:23:45.351646Z","shell.execute_reply.started":"2022-05-25T14:23:45.312117Z","shell.execute_reply":"2022-05-25T14:23:45.35055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" - We can drop the unique identifier for the employees as it is no longer necessary.","metadata":{}},{"cell_type":"code","source":"newdf.drop(columns=['E_ID'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.352931Z","iopub.execute_input":"2022-05-25T14:23:45.353153Z","iopub.status.idle":"2022-05-25T14:23:45.360716Z","shell.execute_reply.started":"2022-05-25T14:23:45.353126Z","shell.execute_reply":"2022-05-25T14:23:45.35958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Unique Values","metadata":{}},{"cell_type":"code","source":"cat_cols = newdf.select_dtypes(include=['object']).columns.tolist()\nfor col in cat_cols:\n    print(\"Unique Values for Column '\",col,\"' :\", newdf[col].unique().tolist())\n    print(\"=\"*148)\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.362478Z","iopub.execute_input":"2022-05-25T14:23:45.363049Z","iopub.status.idle":"2022-05-25T14:23:45.388035Z","shell.execute_reply.started":"2022-05-25T14:23:45.362999Z","shell.execute_reply":"2022-05-25T14:23:45.387062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We saw above what the unique categories are for each feature. Next we will look at the distribution of attrition across these categorical features.","metadata":{}},{"cell_type":"code","source":"for col in cat_cols:\n    if col == 'City' or col=='attrited':\n        continue\n    print(pd.crosstab(newdf[col], newdf[\"attrited\"], normalize='index'))\n    print(\"=\"*148)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.391934Z","iopub.execute_input":"2022-05-25T14:23:45.392299Z","iopub.status.idle":"2022-05-25T14:23:45.508245Z","shell.execute_reply.started":"2022-05-25T14:23:45.392254Z","shell.execute_reply":"2022-05-25T14:23:45.506951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6. Univariate Analysis","metadata":{}},{"cell_type":"markdown","source":"#### 6.a. Target Feature","metadata":{}},{"cell_type":"code","source":"newdf[\"attrited\"].value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.509769Z","iopub.execute_input":"2022-05-25T14:23:45.510066Z","iopub.status.idle":"2022-05-25T14:23:45.519838Z","shell.execute_reply.started":"2022-05-25T14:23:45.510031Z","shell.execute_reply":"2022-05-25T14:23:45.518675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.set_palette(sns.color_palette(\"pastel\"))\nsns.countplot(newdf[\"attrited\"])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.521132Z","iopub.execute_input":"2022-05-25T14:23:45.521391Z","iopub.status.idle":"2022-05-25T14:23:45.714541Z","shell.execute_reply.started":"2022-05-25T14:23:45.521359Z","shell.execute_reply":"2022-05-25T14:23:45.713641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong>So we see that almost 67.8 % of the employees have left the organization.   </strong>\n","metadata":{}},{"cell_type":"markdown","source":"#### 6.b. Numerical Features","metadata":{}},{"cell_type":"code","source":"num_cols = ['Age', 'Salary', 'Total Business Value', 'Experience']\n\n\ncycol = cycle('cbmr')\n\n\nfig, ax = plt.subplots(len(num_cols),2, figsize=(16,20))\n\nfor i in range(len(num_cols)):\n    color_next = next(cycol)\n    sns.distplot(newdf[num_cols[i]], ax=ax[i,0], color=color_next)\n    ax[i,0].set_title(num_cols[i])\n    ax[i,0].set_xlabel('')\n    sns.boxplot(newdf[num_cols[i]], width = 0.3, ax=ax[i,1], color=color_next)\n    ax[i,1].set_title(num_cols[i]+str(\"-Boxplot\"))\n    ax[i,1].set_xlabel('')\n    \n#plt.suptitle(\"Univariate Plots for Numerical Columns\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:45.716156Z","iopub.execute_input":"2022-05-25T14:23:45.716396Z","iopub.status.idle":"2022-05-25T14:23:47.237471Z","shell.execute_reply.started":"2022-05-25T14:23:45.716367Z","shell.execute_reply":"2022-05-25T14:23:47.236781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf[num_cols].describe().T","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:47.23915Z","iopub.execute_input":"2022-05-25T14:23:47.239878Z","iopub.status.idle":"2022-05-25T14:23:47.269454Z","shell.execute_reply.started":"2022-05-25T14:23:47.239837Z","shell.execute_reply":"2022-05-25T14:23:47.268567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" #### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong> The median age of the employees is 33. There are employees as young as 21 years and as old as 58 years.  </strong>\n - <strong> The median income is about 55K. It can range from 10K all the wa to 188K.  </strong>\n - <strong> The median business value generated is 817K. </strong>\n - <strong> The median experience is between 5 to 6 months.   </strong>\n -<strong> It seems that there are outliers. Howver, our data is already small. It might not necessarily be a good idea to drop those outliers. Those may not be because of recording error. </strong>","metadata":{}},{"cell_type":"markdown","source":"#### 6.c. Categorical Features","metadata":{}},{"cell_type":"code","source":"cat_cols = ['Gender', 'Education', 'Raise', 'Joining Designation', 'Current Designation', 'Promoted', 'Quarterly Rating', 'qr_increased']\n\nfig, ax = plt.subplots(4,2, figsize=(16,14))\n\nfor i in range(len(cat_cols)):\n    sns.countplot(newdf[cat_cols[i]], ax=ax[i//2,i%2])\n    ax[i//2,i%2].set_title(cat_cols[i])\n    ax[i//2,i%2].set_xlabel('')\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:47.270675Z","iopub.execute_input":"2022-05-25T14:23:47.271007Z","iopub.status.idle":"2022-05-25T14:23:48.134812Z","shell.execute_reply.started":"2022-05-25T14:23:47.270974Z","shell.execute_reply":"2022-05-25T14:23:48.133853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We look at the city column separately since it has many categories.","metadata":{}},{"cell_type":"code","source":"for col in cat_cols:\n    display(pd.DataFrame(newdf[col].value_counts(normalize=True)))\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:48.136099Z","iopub.execute_input":"2022-05-25T14:23:48.136343Z","iopub.status.idle":"2022-05-25T14:23:48.193035Z","shell.execute_reply.started":"2022-05-25T14:23:48.136313Z","shell.execute_reply":"2022-05-25T14:23:48.192094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf[\"City\"].value_counts()[0:2]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:48.194287Z","iopub.execute_input":"2022-05-25T14:23:48.194651Z","iopub.status.idle":"2022-05-25T14:23:48.204704Z","shell.execute_reply.started":"2022-05-25T14:23:48.194605Z","shell.execute_reply":"2022-05-25T14:23:48.203786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16,6))\nx = newdf['City'].value_counts()[0:30].values\ny = newdf['City'].value_counts()[0:30].index.tolist()\n\nsns.barplot(x = x, y = y, orient='h')\nplt.title('City')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:48.206137Z","iopub.execute_input":"2022-05-25T14:23:48.206777Z","iopub.status.idle":"2022-05-25T14:23:48.594656Z","shell.execute_reply.started":"2022-05-25T14:23:48.206727Z","shell.execute_reply":"2022-05-25T14:23:48.593777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong> So we see that there are 59% male employees and 41% female employees.  </strong>\n - <strong> The percentages of employees with different education levels are almost equal (~33%)  </strong>\n - <strong> 98.2% of the employees did not receive any income increment. Only 1.8% received a raise. </strong>\n - <strong> Almost 43% of the employees joined at lowest designation (1). 34% joined at level 2, 20% at level 3 and below 2% joined at higher levels.   </strong>\n -<strong> Majority (35%) of the employees currently are at designation level 2, followed by designation level 1 (31%) and 3 (26%). Less than 6% of the employees are currently in higher designations. </strong>\n - <strong> Only 17% of the employees received a promotion, while 83% did not. However, we saw that only 1.8% received a raise in income. So it seems like these people are getting ahead in their grade without receiving the financial compensation. This might lead to dissatisfaction. </strong>\n  - <strong>Quarterly Rating is lowest (1) for the majority of employees (73%). Very few received rating over 3 (11.5%). </strong>\n - <strong>Quarterly rating increased for only 15% of employees. It reduced for 19% of the employees and remained unchanged for 65%. </strong>\n - <strong> The majority of the employees seem to be associated with city C20.</strong>","metadata":{}},{"cell_type":"markdown","source":"#### 7. Bivariate Analysis","metadata":{}},{"cell_type":"markdown","source":"#### 7.a. Numerical Columns v/s Attrition","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(4,2, figsize=(16,16))\n\nsns.histplot(newdf[newdf[\"attrited\"]==1][\"Age\"], ax=ax[0][0], label=\"Attrited\", bins=np.arange(20,60,1), color=\"pink\", alpha=0.5)\nsns.histplot(newdf[newdf[\"attrited\"]==0][\"Age\"], ax=ax[0][0], label=\"Not Attrited\", bins=np.arange(20,60,1), color=\"red\", alpha=0.5)\nax[0][0].legend()\n\npal1 = {1: \"pink\", 0: \"red\"}\nsns.boxplot(x=newdf[\"Age\"],y=newdf[\"attrited\"],orient='h', ax=ax[0][1], palette=pal1)\n\nsns.histplot(newdf[newdf[\"attrited\"]==1][\"Salary\"], ax=ax[1][0], label=\"Attrited\", bins=np.arange(20000,200000,2500), color=\"skyblue\", alpha=0.5)\nsns.histplot(newdf[newdf[\"attrited\"]==0][\"Salary\"], ax=ax[1][0],  label=\"Not Attrited\", bins=np.arange(20000,200000,2500), color=\"blue\", alpha=0.5)\nax[1][0].legend()\n\npal2 = {1: \"skyblue\", 0: \"blue\"}\nsns.boxplot(x=newdf[\"Salary\"],y=newdf[\"attrited\"],orient='h', ax=ax[1][1], palette=pal2)\n\nsns.histplot(newdf[newdf[\"attrited\"]==1][\"Total Business Value\"], ax=ax[2][0], label=\"Attrited\", bins=np.arange(20000,200000,20000), color=\"lightgreen\", alpha=0.5)\nsns.histplot(newdf[newdf[\"attrited\"]==0][\"Total Business Value\"], ax=ax[2][0],  label=\"Not Attrited\", bins=np.arange(20000,200000,20000), color=\"orange\", alpha=0.5)\nax[1][0].legend()\n\npal3 = {1: \"lightgreen\", 0: \"orange\"}\nsns.boxplot(x=newdf[\"Total Business Value\"],y=newdf[\"attrited\"],orient='h', ax=ax[2][1], palette=pal3)\n\nsns.histplot(newdf[newdf[\"attrited\"]==1][\"Experience\"], ax=ax[3][0], label=\"Attrited\", bins=np.arange(0,3000,200), color=\"pink\", alpha=0.5)\nsns.histplot(newdf[newdf[\"attrited\"]==0][\"Experience\"], ax=ax[3][0],  label=\"Not Attrited\", bins=np.arange(0,3000,200), color=\"magenta\", alpha=0.5)\nax[1][0].legend()\n\npal4 = {1: \"pink\", 0: \"magenta\"}\nsns.boxplot(x=newdf[\"Experience\"],y=newdf[\"attrited\"],orient='h', ax=ax[3][1], palette=pal4)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:48.596162Z","iopub.execute_input":"2022-05-25T14:23:48.596433Z","iopub.status.idle":"2022-05-25T14:23:50.607462Z","shell.execute_reply.started":"2022-05-25T14:23:48.5964Z","shell.execute_reply":"2022-05-25T14:23:50.606304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf.groupby('attrited')['Age','Salary','Total Business Value', 'Experience'].median()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:50.608889Z","iopub.execute_input":"2022-05-25T14:23:50.609223Z","iopub.status.idle":"2022-05-25T14:23:50.624722Z","shell.execute_reply.started":"2022-05-25T14:23:50.609189Z","shell.execute_reply":"2022-05-25T14:23:50.624079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong> The median age for attrited employees is 33, while for non attrited employees it is 34. </strong>\n - <strong> The median income of atrited employees is lower (\\~51K) than non attrited employees (\\~64K).  </strong>\n - <strong> The median total business value generated by attrited employees (\\~46K) is almost 1/6 of the non attrited employees (\\~263K) </strong>\n - <strong> The median experience for attrited employees is about 165 days and for non attrited employees it is 186 days.   </strong>","metadata":{}},{"cell_type":"markdown","source":"#### 7.b. Categorical Columns vs Attrition","metadata":{}},{"cell_type":"code","source":"for col in cat_cols:\n    d = pd.DataFrame(newdf.groupby([col, 'attrited'])['Salary'].count()).reset_index().rename(columns={'Salary':'percent'})\n    a = d.groupby(col)['percent'].transform('sum')\n    d['percent'] = d['percent'].div(a)\n    d = d[d['attrited']==1]\n    display(d)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:50.626016Z","iopub.execute_input":"2022-05-25T14:23:50.62653Z","iopub.status.idle":"2022-05-25T14:23:50.738895Z","shell.execute_reply.started":"2022-05-25T14:23:50.626492Z","shell.execute_reply":"2022-05-25T14:23:50.73805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(4,2, figsize=(20,20))\n\nsns.countplot(x = \"Gender\", data = newdf, hue = \"attrited\", ax=ax[0][0])\nsns.countplot(x = \"Education\", data = newdf, hue = \"attrited\", ax=ax[0][1])\nsns.countplot(x = \"Raise\", data = newdf, hue = \"attrited\", ax=ax[1][0])\nsns.countplot(x = \"Joining Designation\", data = newdf, hue = \"attrited\", ax=ax[1][1])\nsns.countplot(x = \"Current Designation\", data = newdf, hue = \"attrited\", ax=ax[2][0])\nsns.countplot(x = \"Promoted\", data = newdf, hue = \"attrited\", ax=ax[2][1])\nsns.countplot(x = \"Quarterly Rating\", data = newdf, hue = \"attrited\", ax=ax[3][0])\nsns.countplot(x = \"qr_increased\", data = newdf, hue = \"attrited\", ax=ax[3][1])\nax[1][1].tick_params(axis='x', rotation=90)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:50.740166Z","iopub.execute_input":"2022-05-25T14:23:50.740385Z","iopub.status.idle":"2022-05-25T14:23:51.946106Z","shell.execute_reply.started":"2022-05-25T14:23:50.740356Z","shell.execute_reply":"2022-05-25T14:23:51.945221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong> The attrition rate of male and female employees is similar (\\~67-68%).  </strong>\n - <strong> The attrition rate across different education levels is also similar (from 66-69%).  </strong>\n - <strong> The attrition among those who received raise (7%) is far lower than those who did not receive raise (68%). </strong>\n - <strong> The atrition is highest among those whose current designation is 1 (80%) followed by 2 (70%). The rate is around 50-54% for the remaining designation.   </strong>\n -<strong>The attrition rate is 70% for those who were not promoted and 54% for those who were promoted. </strong>\n - <strong> The attrtion rate follows a clear declinig pattern with increasing quarterly rating : 1 (82%), 2 (40%), 3 (16%) and 4 (9%). </strong>\n - <strong>The attrtion rate is highest among those whose quarterly rating decreased (81%), followed by those whose rating remained unchanged (74%) and finally those whose rating increased (23%). </strong>","metadata":{}},{"cell_type":"markdown","source":"#### 8. Heatmaps to check Correlation","metadata":{}},{"cell_type":"code","source":"sns.heatmap(newdf.corr(), annot=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:51.947713Z","iopub.execute_input":"2022-05-25T14:23:51.94822Z","iopub.status.idle":"2022-05-25T14:23:52.243271Z","shell.execute_reply.started":"2022-05-25T14:23:51.948174Z","shell.execute_reply":"2022-05-25T14:23:52.242567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- <strong>There is mopderate correlation between the Total Business Value and Experience features, which is somewhat expected, since the longer a person works, the more likely it is for him to generate positive business value and higher the sum of monthly business values.</strong>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,8))\ndf1 = newdf.copy()\ncat_cols1 = [\n 'Raise',\n 'Joining Designation',\n 'Current Designation',\n 'Promoted',\n 'Quarterly Rating',\n 'qr_increased',\n 'attrited']\n\nfor col in cat_cols1:\n    df1[col] = df1[col].astype('int64')\n    \nsns.heatmap(df1.corr(), annot=True, cmap='coolwarm')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:52.244871Z","iopub.execute_input":"2022-05-25T14:23:52.245416Z","iopub.status.idle":"2022-05-25T14:23:53.100342Z","shell.execute_reply.started":"2022-05-25T14:23:52.24537Z","shell.execute_reply":"2022-05-25T14:23:53.098862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - <strong> There is moderate correlation between Current Designation and Income which is expected since the higher the designation, the greater the income. </strong>\n - <strong> Also, there is moderate correlation between the current designation and joining designation.</strong>\n - <strong> Again, there is moderate correlation between Experience nd Promotion (the longer a person has been working, the higher the chance of getting promoted. </strong>","metadata":{}},{"cell_type":"code","source":"newdf.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.102222Z","iopub.execute_input":"2022-05-25T14:23:53.104043Z","iopub.status.idle":"2022-05-25T14:23:53.125168Z","shell.execute_reply.started":"2022-05-25T14:23:53.103979Z","shell.execute_reply":"2022-05-25T14:23:53.124035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.126614Z","iopub.execute_input":"2022-05-25T14:23:53.127068Z","iopub.status.idle":"2022-05-25T14:23:53.147366Z","shell.execute_reply.started":"2022-05-25T14:23:53.12703Z","shell.execute_reply":"2022-05-25T14:23:53.146389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id = \"seven\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Encoding Categorical Features + Train-Test Split ◽\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"#### 1. Encoding of Categorical Features","metadata":{}},{"cell_type":"markdown","source":"- Raise, Promoted and qr_increased are already represented as 0 or 1.\n- Joining Designation, Current Designation and Quarterly Rating could be one hot encoded, however, they have inherent order and it would be wise to preserve that order (say somehow 0 and 4 get grouped together while splitting if we treat it as categorical, which may not make logical sense).\n- City could be one hot encoded but there would be too many resultant columns, so we can perform target encoding for it instead.\n- Education can be label encoded (0 : College, 1:Bachelor, 2:Master)\n- Gender can be represented as 0-1 encoding : Male : 0, Female: 1\n","metadata":{}},{"cell_type":"code","source":"newdf['Education'] = newdf['Education'].map({'College':0,'Bachelor':1,'Master':2})\nnewdf['Gender'] = newdf['Gender'].map({'Male':0, 'Female':1})","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.149027Z","iopub.execute_input":"2022-05-25T14:23:53.149534Z","iopub.status.idle":"2022-05-25T14:23:53.160241Z","shell.execute_reply.started":"2022-05-25T14:23:53.149485Z","shell.execute_reply":"2022-05-25T14:23:53.15914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Train Test Split","metadata":{}},{"cell_type":"markdown","source":"We will first separate out the independent and the dependent columns.","metadata":{}},{"cell_type":"code","source":"Y = newdf[\"attrited\"]\nY = Y.astype('int')\nX = newdf.drop(columns=['attrited'])\n\nprint(X.shape)\nprint(Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.161849Z","iopub.execute_input":"2022-05-25T14:23:53.162483Z","iopub.status.idle":"2022-05-25T14:23:53.177468Z","shell.execute_reply.started":"2022-05-25T14:23:53.162431Z","shell.execute_reply":"2022-05-25T14:23:53.176555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we perform train test split.","metadata":{}},{"cell_type":"code","source":"\n\n#X = np.array(X.values.tolist())\n#Y = np.array(Y.values.tolist())\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=1001)\nprint(f\"Sizes of the sets created are:\\nTraining set:{X_train.shape[0]}\\nTest set:{X_test.shape[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.179021Z","iopub.execute_input":"2022-05-25T14:23:53.179492Z","iopub.status.idle":"2022-05-25T14:23:53.196488Z","shell.execute_reply.started":"2022-05-25T14:23:53.179446Z","shell.execute_reply":"2022-05-25T14:23:53.195465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we see that we have around 1904 training observations and 477 test observations.","metadata":{}},{"cell_type":"markdown","source":"#### 3. Target Encoding\nNext, we perform target encoding. We fit the encoder on the training data. We use the same encoder to transform our test data (we do not fit on test data so that there is no 'leakage' of target values of the test data).","metadata":{}},{"cell_type":"code","source":"import category_encoders as ce\n\nce_target = ce.TargetEncoder(cols=['City'])\n\nX_train = ce_target.fit_transform(X_train, Y_train)\nX_test = ce_target.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.19786Z","iopub.execute_input":"2022-05-25T14:23:53.198444Z","iopub.status.idle":"2022-05-25T14:23:53.246234Z","shell.execute_reply.started":"2022-05-25T14:23:53.198391Z","shell.execute_reply":"2022-05-25T14:23:53.244946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.247642Z","iopub.execute_input":"2022-05-25T14:23:53.247927Z","iopub.status.idle":"2022-05-25T14:23:53.269633Z","shell.execute_reply.started":"2022-05-25T14:23:53.247892Z","shell.execute_reply":"2022-05-25T14:23:53.268911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we shall check the class imbalance of our sets.","metadata":{}},{"cell_type":"code","source":"Y_train.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.271149Z","iopub.execute_input":"2022-05-25T14:23:53.271588Z","iopub.status.idle":"2022-05-25T14:23:53.28115Z","shell.execute_reply.started":"2022-05-25T14:23:53.271547Z","shell.execute_reply":"2022-05-25T14:23:53.2798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.282864Z","iopub.execute_input":"2022-05-25T14:23:53.283217Z","iopub.status.idle":"2022-05-25T14:23:53.300539Z","shell.execute_reply.started":"2022-05-25T14:23:53.283172Z","shell.execute_reply":"2022-05-25T14:23:53.299539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>So we see that there is similar distribution of attrition in both the train and test data. So we have class imbalance. We have to treat it while we train our models.</strong>","metadata":{}},{"cell_type":"markdown","source":"<a id = \"eight\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Simple Decision Tree Classifier ◽\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"### 1. Simple Decision Tree Classifier","metadata":{}},{"cell_type":"markdown","source":" - We will use 5 fold Cross Validation.\n - We will use Stratified K-Fold. Our training Data is only about 1904 observations in size. Of these, we have around 1286 positive class observations and 618 negative class observations, which turns out to be a 68:32 ratio.. If we perform 5 fold stratified CV on this, we should roughly get 257 positive and 124 negative points. Since the size is small, if we choose the folds randomly, we might end up with different ratio than expected, in each of the folds.\n - We will use GridSearch to tune the hyperparameters, since we are training a single decision tree on a small data and it will not take a lot of time.\n - We will treat class imbalance in the following ways:\n     - Using class weights : We will keep this as a hyperparameter\n     - Using oversampling (by SMOTE)\n     - We will NOT use undersampling since the data size is already small.\n - We will consider maximum tree depth, and maximum number of leaf nodes as the other hyperparameters.\n ","metadata":{}},{"cell_type":"markdown","source":"#### 1.a. Treating imbalance using Class Weights","metadata":{}},{"cell_type":"code","source":"cl1 = Y_train.value_counts().values[0]\ncl0 = Y_train.value_counts().values[1]\nr0 = 1\nr1 = round(cl1/cl0,2)\nprint(\"Ratio of class0 to class1 is\", r0, \":\", r1)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.302279Z","iopub.execute_input":"2022-05-25T14:23:53.302994Z","iopub.status.idle":"2022-05-25T14:23:53.314755Z","shell.execute_reply.started":"2022-05-25T14:23:53.302946Z","shell.execute_reply":"2022-05-25T14:23:53.313869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#We could also treat weights as hyperparameters.\nweights = {0:r1, 1:r0}\nmodel1 = DecisionTreeClassifier()\n\n#hyper parameters which we will tune using GridSearch\nparams = {\n    \"class_weight\" : [{0:1, 1:1}, {0:r1, 1:r0}, {0:3, 1:1},],\n    \"max_depth\" : [5, 7, 10], #the maximum depth of the tree\n    \"max_leaf_nodes\" : [20, 25, 30] #the number of leaf nodes of the tree\n}\n\n#stratified k fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 1001)\n#specifying our classifier\nclf = GridSearchCV(model1, params, scoring = \"f1\", cv=skf.split(X_train,Y_train))\n#Training the model\nclf.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:53.316587Z","iopub.execute_input":"2022-05-25T14:23:53.317356Z","iopub.status.idle":"2022-05-25T14:23:55.244395Z","shell.execute_reply.started":"2022-05-25T14:23:53.317308Z","shell.execute_reply":"2022-05-25T14:23:55.243456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#The classifier with the best hyper parameters\nprint(clf.best_estimator_)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:55.245631Z","iopub.execute_input":"2022-05-25T14:23:55.2459Z","iopub.status.idle":"2022-05-25T14:23:55.252336Z","shell.execute_reply.started":"2022-05-25T14:23:55.245867Z","shell.execute_reply":"2022-05-25T14:23:55.251101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>It turns out that the original class weighting still gives us better performance. </strong>","metadata":{}},{"cell_type":"markdown","source":"#### Learning Curves\nNext we will look at the learning curve. It plots the 'Score' obtained from various models trained on different sizes of the training data. \nPlease look at https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html for details on other possible plots.","metadata":{}},{"cell_type":"code","source":"# Learning Curves\n\n\ndef plot_learning_curve(estimator, X, Y, title):\n\n    train_sizes, train_scores, test_scores, _, _ = learning_curve(estimator,X,Y,return_times=True)\n\n    fig, axes = plt.subplots(1, 1, figsize = (10, 5))\n\n    axes.set_title(title)\n    axes.plot\n    axes.set_xlabel(\"Training examples\")\n    axes.set_ylabel(\"Score\")\n\n\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n\n    # Plot learning curve\n    axes.grid()\n    axes.fill_between(\n      train_sizes,\n      train_scores_mean - train_scores_std,\n      train_scores_mean + train_scores_std,\n      alpha=0.1,\n      color=\"r\",\n    )\n    axes.fill_between(\n      train_sizes,\n      test_scores_mean - test_scores_std,\n      test_scores_mean + test_scores_std,\n      alpha=0.1,\n      color=\"g\",\n    )\n    axes.plot(\n      train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n      )\n    axes.plot(\n      train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n      )\n    axes.legend(loc=\"best\")\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:55.253781Z","iopub.execute_input":"2022-05-25T14:23:55.254104Z","iopub.status.idle":"2022-05-25T14:23:55.270271Z","shell.execute_reply.started":"2022-05-25T14:23:55.25403Z","shell.execute_reply":"2022-05-25T14:23:55.269374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model1 = clf.best_estimator_\n\nmodel1.fit(X_train, Y_train)\n\nplot_learning_curve(model1, X_train, Y_train, \"Decision Trees\")\n\nprint(model1.score(X_train, Y_train))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:55.272011Z","iopub.execute_input":"2022-05-25T14:23:55.272744Z","iopub.status.idle":"2022-05-25T14:23:55.879914Z","shell.execute_reply.started":"2022-05-25T14:23:55.272699Z","shell.execute_reply":"2022-05-25T14:23:55.878926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - So we see that the training score is very high at the beginning and decreases and the cross-validation score is low at the beginning and increases.\n\n- In the beginning, with very few samples our model is overfitting. As the number of samples increases, our model starts generalizing better and our CV score improves. \n\n- We see that our training and CV scores appear to be close at 1900 samples. We might be able to get only a slight improvement with more training samples, but not too much.\n\n- The highest cross validation score obtained was around 0.84.","metadata":{}},{"cell_type":"markdown","source":"#### Visualizing what the tree looks like\nNext, we could also visualize the tree that we have trained.","metadata":{}},{"cell_type":"code","source":"\n\nplt.figure(figsize=(24,12))  # set plot size (denoted in inches)\ntree.plot_tree(model1, fontsize=10)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:55.881158Z","iopub.execute_input":"2022-05-25T14:23:55.88142Z","iopub.status.idle":"2022-05-25T14:23:57.799337Z","shell.execute_reply.started":"2022-05-25T14:23:55.881388Z","shell.execute_reply":"2022-05-25T14:23:57.797995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Test Performance","metadata":{}},{"cell_type":"code","source":"print(f\"Training F1 score:{model1.score(X_train, Y_train)}\\nTest F1 score: {model1.score(X_test, Y_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:57.800779Z","iopub.execute_input":"2022-05-25T14:23:57.801066Z","iopub.status.idle":"2022-05-25T14:23:57.818873Z","shell.execute_reply.started":"2022-05-25T14:23:57.801033Z","shell.execute_reply":"2022-05-25T14:23:57.817798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 1.b. Treating Class Imbalance using SMOTE Oversampling","metadata":{}},{"cell_type":"code","source":"smote = SMOTE(random_state=1001)\nX_train_ov, Y_train_ov = smote.fit_resample(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:57.821322Z","iopub.execute_input":"2022-05-25T14:23:57.821586Z","iopub.status.idle":"2022-05-25T14:23:57.841771Z","shell.execute_reply.started":"2022-05-25T14:23:57.821549Z","shell.execute_reply":"2022-05-25T14:23:57.84092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train_ov.value_counts() #We now have equal observations in both classes,","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:57.843218Z","iopub.execute_input":"2022-05-25T14:23:57.843731Z","iopub.status.idle":"2022-05-25T14:23:57.852947Z","shell.execute_reply.started":"2022-05-25T14:23:57.843683Z","shell.execute_reply":"2022-05-25T14:23:57.852268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#stratified k fold\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state = 1001)\n\nmodel1_ov = DecisionTreeClassifier()\n\n#parameters which we will tune using GridSearch. We do not need class weights as we have balanced our data.\nparams_ov = {\n    \"max_depth\" : [3, 5, 7], #the maximum depth of the tree\n    \"max_leaf_nodes\" : [15, 20, 25] #the number of leaf nodes of the tree\n}\n\n#specifying our classifier\nclf_ov = GridSearchCV(model1_ov, params_ov, scoring = \"f1\", cv=skf.split(X_train,Y_train))\n#Training the model\nclf_ov.fit(X_train_ov, Y_train_ov)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:57.854038Z","iopub.execute_input":"2022-05-25T14:23:57.854863Z","iopub.status.idle":"2022-05-25T14:23:58.636355Z","shell.execute_reply.started":"2022-05-25T14:23:57.854798Z","shell.execute_reply":"2022-05-25T14:23:58.635379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Learning Curve","metadata":{}},{"cell_type":"code","source":"model1_ov = clf_ov.best_estimator_\n\nmodel1_ov.fit(X_train_ov, Y_train_ov)\n\nplot_learning_curve(model1_ov, X_train_ov, Y_train_ov, \"Decision Trees - with SMOTE Oversampling\")\n\nprint(model1_ov.score(X_train_ov, Y_train_ov))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:58.639714Z","iopub.execute_input":"2022-05-25T14:23:58.640097Z","iopub.status.idle":"2022-05-25T14:23:59.25665Z","shell.execute_reply.started":"2022-05-25T14:23:58.64004Z","shell.execute_reply":"2022-05-25T14:23:59.25573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- We obtain a similar learning curve as before, but looks like our CV score is still increasing but only a little. The highest cross validation score obtained was around 0.83 - 0.84.","metadata":{}},{"cell_type":"markdown","source":"#### Test Performace","metadata":{}},{"cell_type":"code","source":"print(f\"Training F1 score:{model1_ov.score(X_train_ov, Y_train_ov)}\\nTest F1 score: {model1_ov.score(X_test, Y_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:59.25803Z","iopub.execute_input":"2022-05-25T14:23:59.258272Z","iopub.status.idle":"2022-05-25T14:23:59.275025Z","shell.execute_reply.started":"2022-05-25T14:23:59.258241Z","shell.execute_reply":"2022-05-25T14:23:59.274041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparing Test Performance and Feature Importance of the two Decision Tree Classifiers","metadata":{}},{"cell_type":"markdown","source":"#### 1. Feature Importance","metadata":{}},{"cell_type":"code","source":"dt_feat_imp = model1.feature_importances_\ndt_feat_imp_ov = model1_ov.feature_importances_\n\nfig, ax = plt.subplots(1,2, figsize=(22, 5))\n\nsns.barplot(y = list(X_train.columns), x = dt_feat_imp, orient='h', ax=ax[0])\nax[0].set_title(\"Feature Importances - DT\")\nsns.barplot(y = list(X_train.columns), x = dt_feat_imp_ov, orient='h', ax=ax[1])\nax[1].set_title(\"Feature Importances - DT with Oversampling\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:59.276402Z","iopub.execute_input":"2022-05-25T14:23:59.276718Z","iopub.status.idle":"2022-05-25T14:23:59.715925Z","shell.execute_reply.started":"2022-05-25T14:23:59.276674Z","shell.execute_reply":"2022-05-25T14:23:59.714976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Precision, Recall and F1 scores\n\nWe already saw that the F1 score on test data was higher for the base model without oversampling.\nWe will check the classification reports for the two models.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, precision_recall_curve, auc\nY_test_pred = model1.predict(X_test)\nY_test_pred_ov = model1_ov.predict(X_test)\nprint(\"DT model:\")\nprint(classification_report(Y_test, Y_test_pred))\nprint(\"-\"*60)\nprint(\"\\nDT model with oversampling :\")\nprint(classification_report(Y_test, Y_test_pred_ov))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:59.717258Z","iopub.execute_input":"2022-05-25T14:23:59.717511Z","iopub.status.idle":"2022-05-25T14:23:59.741213Z","shell.execute_reply.started":"2022-05-25T14:23:59.717479Z","shell.execute_reply":"2022-05-25T14:23:59.740149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - We look at the precision, recall and F1 scores for the '1' (attrited) class, since that is what we are trying to predict.\n\n- So the base model has Precision, Recall and F1 score of 0.81, 0.93 and 0.86 respectively.\n- And the model with oversampling has the values as 0.84, 0.83 and 0.83 respectively.\n\n- <strong>So the precision is higher for the second model, but the recall and F1 score are better for the base model which used class weights as hyper parameters. </strong>","metadata":{}},{"cell_type":"markdown","source":"#### 3. Accuracy and Confusion Matrices","metadata":{}},{"cell_type":"code","source":"print(\"Test accuracy for simple DT -\", accuracy_score(Y_test, Y_test_pred))\nprint(\"Test accuracy for DT with oversampling -\", accuracy_score(Y_test, Y_test_pred_ov))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:59.742542Z","iopub.execute_input":"2022-05-25T14:23:59.743482Z","iopub.status.idle":"2022-05-25T14:23:59.751301Z","shell.execute_reply.started":"2022-05-25T14:23:59.743433Z","shell.execute_reply":"2022-05-25T14:23:59.750192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion matrix format:\")\nprint(\"   0   1\")\nprint(\"   _____\")\nprint(\"0 |__|__|\")\nprint(\"1 |__|__|\")\nprint(\"-\"*60)\n\nprint(\"\\nDT model:\")\nprint(confusion_matrix(Y_test, Y_test_pred))\nprint(\"-\"*60)\nprint(\"\\nDT model with oversampling :\")\nprint(confusion_matrix(Y_test, Y_test_pred_ov))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:59.752531Z","iopub.execute_input":"2022-05-25T14:23:59.753356Z","iopub.status.idle":"2022-05-25T14:23:59.769957Z","shell.execute_reply.started":"2022-05-25T14:23:59.753306Z","shell.execute_reply":"2022-05-25T14:23:59.768882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>So once again we see that the base DT seems to be performng better w.r.t prediction of positive class and also has better accuracy.</strong>","metadata":{}},{"cell_type":"markdown","source":"#### 4. ROC Curves, Area under ROC Curves","metadata":{}},{"cell_type":"code","source":"probs_y=model1.predict_proba(X_test) \nprecision, recall, thresholds = precision_recall_curve(Y_test, probs_y[:, 1]) \nprobs_y_ov=model1_ov.predict_proba(X_test) \nprecision_ov, recall_ov, thresholds_ov = precision_recall_curve(Y_test, probs_y_ov[:, 1]) \n\n\nfpr, tpr, thresholds = roc_curve(Y_test, probs_y[:,1], drop_intermediate=False)\nroc_auc = auc(fpr, tpr)\nfpr_ov, tpr_ov, thresholds_ov = roc_curve(Y_test, probs_y_ov[:,1], drop_intermediate=False)\nroc_auc_ov = auc(fpr_ov, tpr_ov)\n\nfig, ax = plt.subplots(1,2, figsize=(16,4))\n\nplt.suptitle('Receiver Operating Characteristics')\n\nax[0].plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax[0].legend(loc = 'lower right')\nax[0].plot([0, 1], [0, 1],'r--')\nax[0].set_xlim([0, 1])\nax[0].set_ylim([0, 1])\nax[0].set_ylabel('True Positive Rate')\nax[0].set_xlabel('False Positive Rate')\nax[0].set_title(\"DT\")\n\n\nax[1].plot(fpr_ov, tpr_ov, 'g', label = 'AUC = %0.2f' % roc_auc_ov)\nax[1].legend(loc = 'lower right')\nax[1].plot([0, 1], [0, 1],'r--')\nax[1].set_xlim([0, 1])\nax[1].set_ylim([0, 1])\nax[1].set_ylabel('True Positive Rate')\nax[1].set_xlabel('False Positive Rate')\nax[1].set_title(\"DT with oversampling\")\n\n\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:23:59.77118Z","iopub.execute_input":"2022-05-25T14:23:59.771409Z","iopub.status.idle":"2022-05-25T14:24:00.125455Z","shell.execute_reply.started":"2022-05-25T14:23:59.77138Z","shell.execute_reply":"2022-05-25T14:24:00.124443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>So we see that the base DT model and the oversampled data model have same AUC of 0.83</strong>\n\n<strong>So from <mark>Section 1</mark>, the best model seems to be : <mark>model1</mark></strong>\n#### ","metadata":{}},{"cell_type":"markdown","source":"<a id = \"nine\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Gradient Boosted Decision Trees using XGBoost ◽\n    </center>\n</h1>","metadata":{"execution":{"iopub.status.busy":"2022-05-25T13:08:19.677159Z","iopub.execute_input":"2022-05-25T13:08:19.679239Z","iopub.status.idle":"2022-05-25T13:08:19.698167Z","shell.execute_reply.started":"2022-05-25T13:08:19.679095Z","shell.execute_reply":"2022-05-25T13:08:19.695Z"}}},{"cell_type":"markdown","source":"### 2. Gradient Boosted Decision Trees (XgBoost)\n\n - We will again use 5 fold Cross Validation.\n - We will use Stratified K-Fold.\n - We will use GridSearch and try out RandomSearch to tune the hyperparameters. If GridSearch ends up taking a lot of time to  train, we might go with RandomSearch. If it manages to finish within reasonable time, we can stick with GridSearch.\n - We will treat class imbalance in the following ways:\n     - Using class weights : We will keep this as a hyperparameter\n     - Using oversampling (by SMOTE)\n - We will consider maximum tree depth, learning rate, row sampling and column sampling rates as the other hyperparameters.\n - We will use 100 base learners.","metadata":{}},{"cell_type":"markdown","source":"#### 2.a. Using Class Weights\n\n- According to the XGBoost documentation, a typical value to consider to balance the positive and negative weights is:\n\nsum(negative instances) / sum(positive instances)\n\n- scale_pos_weight = total_negative_examples / total_positive_examples","metadata":{}},{"cell_type":"code","source":"Y_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:24:00.126799Z","iopub.execute_input":"2022-05-25T14:24:00.127489Z","iopub.status.idle":"2022-05-25T14:24:00.137995Z","shell.execute_reply.started":"2022-05-25T14:24:00.127436Z","shell.execute_reply":"2022-05-25T14:24:00.135657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Random Search\nWe will first look at Random Search.","metadata":{}},{"cell_type":"code","source":"\n\nfor col in X_train.columns:\n    X_train[col] = X_train[col].astype('int')\n    X_test[col] = X_test[col].astype('int')\n\nY_train = Y_train.astype('int')\nY_test = Y_test.astype('int')\n\n\nscale_pos_w = 606/1298\n#hyper-parameters\nparams = {\n        'scale_pos_weight' : [0.5, 1, scale_pos_w, 5],\n        'learning_rate': [0.1, 0.5, 0.8],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\n#XgBoost Classifier\nxgb = XGBClassifier(n_estimators=100, objective='binary:logistic')\n\nfolds = 5\n#Stratified k-folds\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 1001)\n\n#model with random search for hyper parameter tuning\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=50, scoring='f1', n_jobs=4, cv=skf.split(X_train,Y_train), verbose=0, random_state=1001)\n\n#Training\nstart = dt.datetime.now()\nrandom_search.fit(X_train, Y_train)\nend = dt.datetime.now()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:24:00.139781Z","iopub.execute_input":"2022-05-25T14:24:00.140107Z","iopub.status.idle":"2022-05-25T14:24:37.763923Z","shell.execute_reply.started":"2022-05-25T14:24:00.140071Z","shell.execute_reply":"2022-05-25T14:24:37.763038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Model obtained by Random Search","metadata":{}},{"cell_type":"code","source":"print('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nrandom_best = random_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:24:37.765444Z","iopub.execute_input":"2022-05-25T14:24:37.769289Z","iopub.status.idle":"2022-05-25T14:24:37.777789Z","shell.execute_reply.started":"2022-05-25T14:24:37.769234Z","shell.execute_reply":"2022-05-25T14:24:37.776628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n - So just like the simple DT, we once again end up obtaining best results with original class weights.","metadata":{}},{"cell_type":"code","source":"subsample = random_best['subsample']\nmax_depth = random_best['max_depth']\nlearning_rate = random_best['learning_rate']\ncolsample_bytree = random_best['colsample_bytree']\nscale_pos_weight = random_best['scale_pos_weight']\n\nbest_xgb_random = XGBClassifier(n_estimators=100, objective='binary:logistic', subsample=subsample, max_depth=max_depth, learning_rate=learning_rate, colsample_bytree=colsample_bytree, scale_pos_weight=scale_pos_weight)\nbest_xgb_random.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:24:37.77921Z","iopub.execute_input":"2022-05-25T14:24:37.779494Z","iopub.status.idle":"2022-05-25T14:24:38.14767Z","shell.execute_reply.started":"2022-05-25T14:24:37.77946Z","shell.execute_reply":"2022-05-25T14:24:38.146925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Learning Curve","metadata":{}},{"cell_type":"code","source":"plot_learning_curve(best_xgb_random, X_train, Y_train, \"XgBoost with Random Search\")","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-25T14:24:38.15117Z","iopub.execute_input":"2022-05-25T14:24:38.152235Z","iopub.status.idle":"2022-05-25T14:24:46.4147Z","shell.execute_reply.started":"2022-05-25T14:24:38.152184Z","shell.execute_reply":"2022-05-25T14:24:46.413796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- The highest cross validation score obtained was around 0.85. It might improve with more training samples.","metadata":{}},{"cell_type":"markdown","source":"#### Test Performance","metadata":{}},{"cell_type":"code","source":"print(f\"Time taken for training : {end - start}\\nTraining F1 score:{best_xgb_random.score(X_train, Y_train)}\\nTest F1 score: {best_xgb_random.score(X_test, Y_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:24:46.416064Z","iopub.execute_input":"2022-05-25T14:24:46.416951Z","iopub.status.idle":"2022-05-25T14:24:46.440512Z","shell.execute_reply.started":"2022-05-25T14:24:46.416901Z","shell.execute_reply":"2022-05-25T14:24:46.439835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Grid Search","metadata":{}},{"cell_type":"code","source":"#hyper parameters for grid search\nparams = {\n        'scale_pos_weight' : [0.5, 1, scale_pos_w, 5],\n        'learning_rate': [0.1, 0.5, 0.8],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\n#XgBoost Classifier\nxgb = XGBClassifier(n_estimators=100, objective='binary:logistic')\n\nfolds = 5\n#Stratified k-folds\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 1001)\n\n#model with grid search for hyperparameter tuning\ngrid_search = GridSearchCV(xgb, params, scoring='f1', n_jobs=4, cv=skf.split(X_train,Y_train))\n\n\nstart = dt.datetime.now()\ngrid_search.fit(X_train, Y_train)\nend = dt.datetime.now()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:24:46.443504Z","iopub.execute_input":"2022-05-25T14:24:46.446204Z","iopub.status.idle":"2022-05-25T14:28:33.241516Z","shell.execute_reply.started":"2022-05-25T14:24:46.446153Z","shell.execute_reply":"2022-05-25T14:28:33.240833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best model obtained by Grid Search","metadata":{}},{"cell_type":"code","source":"print('\\n Best hyperparameters:')\nprint(grid_search.best_params_)\ngrid_best = grid_search.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:28:33.242866Z","iopub.execute_input":"2022-05-25T14:28:33.243395Z","iopub.status.idle":"2022-05-25T14:28:33.249562Z","shell.execute_reply.started":"2022-05-25T14:28:33.243355Z","shell.execute_reply":"2022-05-25T14:28:33.248779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subsample = grid_best['subsample']\nmax_depth = grid_best['max_depth']\nlearning_rate = grid_best['learning_rate']\ncolsample_bytree = grid_best['colsample_bytree']\nscale_pos_weight = random_best['scale_pos_weight']\n\nbest_xgb_grid = XGBClassifier(n_estimators=100, objective='binary:logistic', subsample=subsample, max_depth=max_depth, learning_rate=learning_rate, colsample_bytree=colsample_bytree, scale_pos_weight=scale_pos_weight)\nbest_xgb_grid.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:28:33.251033Z","iopub.execute_input":"2022-05-25T14:28:33.25158Z","iopub.status.idle":"2022-05-25T14:28:33.63134Z","shell.execute_reply.started":"2022-05-25T14:28:33.251537Z","shell.execute_reply":"2022-05-25T14:28:33.630579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Learning Curve","metadata":{}},{"cell_type":"code","source":"plot_learning_curve(best_xgb_grid, X_train, Y_train, \"XgBoost with Grid Search\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:28:33.635188Z","iopub.execute_input":"2022-05-25T14:28:33.635954Z","iopub.status.idle":"2022-05-25T14:28:41.98871Z","shell.execute_reply.started":"2022-05-25T14:28:33.635915Z","shell.execute_reply":"2022-05-25T14:28:41.987479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- The highest cross validation score obtained was slightly above 0.85.","metadata":{}},{"cell_type":"markdown","source":"#### Test Performance","metadata":{}},{"cell_type":"code","source":"print(f\"Time taken for training : {end - start}\\nTraining F1 score:{best_xgb_grid.score(X_train, Y_train)}\\nTest F1 score: {best_xgb_grid.score(X_test, Y_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:28:41.990129Z","iopub.execute_input":"2022-05-25T14:28:41.990373Z","iopub.status.idle":"2022-05-25T14:28:42.010411Z","shell.execute_reply.started":"2022-05-25T14:28:41.99034Z","shell.execute_reply":"2022-05-25T14:28:42.009727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>So, it appears that the performance of the model obtained by random search, on test data, is better than that obtained by grid search (even though the best CV scores for Grid Search model is only slightly higher than that of the Random Search Model.</strong>\n\n<strong>We will pick the <mark>random search model</mark> from section 2.a. for our later comparisons. </strong>","metadata":{}},{"cell_type":"markdown","source":"#### 2.b. Model on Oversampled Data","metadata":{}},{"cell_type":"markdown","source":"#### Random Search","metadata":{}},{"cell_type":"code","source":"for col in X_train_ov.columns:\n    X_train_ov[col] = X_train_ov[col].astype('int')\nY_train_ov = Y_train_ov.astype('int')\n\n#hyper-parameters\nparams = {\n        'learning_rate': [0.1, 0.5, 0.8],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\n#XgBoost Classifier\nxgb = XGBClassifier(n_estimators=100, objective='binary:logistic')\n\nfolds = 5\n#Stratified k-folds\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 1001)\n\n#model with random search for hyper parameter tuning\nrandom_search_ov = RandomizedSearchCV(xgb, param_distributions=params, n_iter=50, scoring='f1', n_jobs=4, cv=skf.split(X_train_ov,Y_train_ov), verbose=0, random_state=1001 )\n\n#Training\nstart = dt.datetime.now()\nrandom_search_ov.fit(X_train_ov, Y_train_ov)\nend = dt.datetime.now()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:28:42.011698Z","iopub.execute_input":"2022-05-25T14:28:42.012097Z","iopub.status.idle":"2022-05-25T14:29:21.663481Z","shell.execute_reply.started":"2022-05-25T14:28:42.012064Z","shell.execute_reply":"2022-05-25T14:29:21.662767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Model obtained by Random Search","metadata":{}},{"cell_type":"code","source":"print('\\n Best hyperparameters:')\nprint(random_search_ov.best_params_)\nrandom_best_ov = random_search_ov.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:29:21.664929Z","iopub.execute_input":"2022-05-25T14:29:21.665433Z","iopub.status.idle":"2022-05-25T14:29:21.672877Z","shell.execute_reply.started":"2022-05-25T14:29:21.665387Z","shell.execute_reply":"2022-05-25T14:29:21.672178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subsample = random_best_ov['subsample']\nmax_depth = random_best_ov['max_depth']\nlearning_rate = random_best_ov['learning_rate']\ncolsample_bytree = random_best_ov['colsample_bytree']\n\nbest_xgb_random_ov = XGBClassifier(n_estimators=100, objective='binary:logistic', subsample=subsample, max_depth=max_depth, learning_rate=learning_rate, colsample_bytree=colsample_bytree)\nbest_xgb_random_ov.fit(X_train_ov, Y_train_ov)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:29:21.674195Z","iopub.execute_input":"2022-05-25T14:29:21.674688Z","iopub.status.idle":"2022-05-25T14:29:22.182521Z","shell.execute_reply.started":"2022-05-25T14:29:21.674651Z","shell.execute_reply":"2022-05-25T14:29:22.181744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Time taken for training : {end - start}\\nTraining F1 score:{best_xgb_random_ov.score(X_train_ov, Y_train_ov)}\\nTest F1 score: {best_xgb_random_ov.score(X_test, Y_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:29:22.186556Z","iopub.execute_input":"2022-05-25T14:29:22.186863Z","iopub.status.idle":"2022-05-25T14:29:22.208156Z","shell.execute_reply.started":"2022-05-25T14:29:22.1868Z","shell.execute_reply":"2022-05-25T14:29:22.207297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Grid Search","metadata":{}},{"cell_type":"code","source":"#hyper parameters for grid search\nparams = {\n        'learning_rate': [0.1, 0.5, 0.8],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n\n#XgBoost Classifier\nxgb = XGBClassifier(n_estimators=100, objective='binary:logistic')\n\n#Stratified k-folds\nfolds = 5\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 1001)\n\n#model with grid search for hyperparameter tuning\ngrid_search_ov = GridSearchCV(xgb, params, scoring='f1', n_jobs=4, cv=skf.split(X_train_ov,Y_train_ov))\n\n\nstart = dt.datetime.now()\ngrid_search_ov.fit(X_train_ov, Y_train_ov)\nend = dt.datetime.now()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:29:22.211957Z","iopub.execute_input":"2022-05-25T14:29:22.21427Z","iopub.status.idle":"2022-05-25T14:30:25.674354Z","shell.execute_reply.started":"2022-05-25T14:29:22.214219Z","shell.execute_reply":"2022-05-25T14:30:25.673656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('\\n Best hyperparameters:')\nprint(grid_search_ov.best_params_)\ngrid_best_ov = grid_search_ov.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:30:25.676084Z","iopub.execute_input":"2022-05-25T14:30:25.676697Z","iopub.status.idle":"2022-05-25T14:30:25.684018Z","shell.execute_reply.started":"2022-05-25T14:30:25.67665Z","shell.execute_reply":"2022-05-25T14:30:25.683285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- <strong>So we see that both Random and Grid Search gives us the same model.</strong>","metadata":{}},{"cell_type":"markdown","source":"### Comparing Test Performance and Feature Importance of the two XgBoost Ensembles","metadata":{}},{"cell_type":"markdown","source":"- So we pick the random search model from 2.a <mark>(best_xgb_random)</mark> and the model from 2.b <mark>(best_xgb_random_ov)</mark> for our comparison.","metadata":{}},{"cell_type":"markdown","source":"#### Feature Importances","metadata":{}},{"cell_type":"code","source":"xgb_feat_imp = best_xgb_random.feature_importances_\nxgb_feat_imp_ov = best_xgb_random_ov.feature_importances_\n\nfig, ax = plt.subplots(1,2, figsize=(22, 5))\n\nsns.barplot(y = list(X_train.columns), x = xgb_feat_imp, orient='h', ax=ax[0])\nax[0].set_title(\"Feature Importances - XGBoost\")\nsns.barplot(y = list(X_train.columns), x = xgb_feat_imp_ov, orient='h', ax=ax[1])\nax[1].set_title(\"Feature Importances - XGBoost with Oversampling\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:30:25.685152Z","iopub.execute_input":"2022-05-25T14:30:25.686011Z","iopub.status.idle":"2022-05-25T14:30:26.161577Z","shell.execute_reply.started":"2022-05-25T14:30:25.685961Z","shell.execute_reply":"2022-05-25T14:30:26.160877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Precision, Recall and F1 scores\n\nWe already saw that the F1 score on test data was higher for the base model without oversampling.\nWe will check the classification reports for the two models.","metadata":{}},{"cell_type":"code","source":"Y_test_pred = best_xgb_random.predict(X_test)\nY_test_pred_ov = best_xgb_random_ov.predict(X_test)\nprint(\"XgBoost model:\")\nprint(classification_report(Y_test, Y_test_pred))\nprint(\"-\"*60)\nprint(\"\\nXgBoost model with oversampling :\")\nprint(classification_report(Y_test, Y_test_pred_ov))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:30:26.162748Z","iopub.execute_input":"2022-05-25T14:30:26.163132Z","iopub.status.idle":"2022-05-25T14:30:26.194198Z","shell.execute_reply.started":"2022-05-25T14:30:26.163091Z","shell.execute_reply":"2022-05-25T14:30:26.193527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>We look at the precision, recall and F1 scores for the '1' (attrited) class, since that is what we are trying to predict.</strong>\n\n- <strong>So the base model has Precision, Recall and F1 score of 0.84, 0.91 and 0.87 respectively.</strong>\n- <strong>And the model with oversampling has the values as 0.85, 0.85 and 0.85 respectively.</strong>\n\n- <strong>So the precision is higher for the second model, but the recall and F1 score are better for the base xgboost model which used class weights as hyper parameters.</strong>","metadata":{}},{"cell_type":"markdown","source":"#### Accuracy and Confusion Matrices","metadata":{}},{"cell_type":"code","source":"print(\"Test accuracy for XGBoost -\", accuracy_score(Y_test, Y_test_pred))\nprint(\"Test accuracy for XGBoost with oversampling -\", accuracy_score(Y_test, Y_test_pred_ov))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:30:26.195562Z","iopub.execute_input":"2022-05-25T14:30:26.196067Z","iopub.status.idle":"2022-05-25T14:30:26.202499Z","shell.execute_reply.started":"2022-05-25T14:30:26.196029Z","shell.execute_reply":"2022-05-25T14:30:26.201431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion matrix format:\")\nprint(\"   0   1\")\nprint(\"   _____\")\nprint(\"0 |__|__|\")\nprint(\"1 |__|__|\")\nprint(\"-\"*60)\n\nprint(\"\\nXGBoost model:\")\nprint(confusion_matrix(Y_test, Y_test_pred))\nprint(\"-\"*60)\nprint(\"\\nXGBoost model with oversampling :\")\nprint(confusion_matrix(Y_test, Y_test_pred_ov))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-25T14:30:26.204378Z","iopub.execute_input":"2022-05-25T14:30:26.204731Z","iopub.status.idle":"2022-05-25T14:30:26.222601Z","shell.execute_reply.started":"2022-05-25T14:30:26.204699Z","shell.execute_reply":"2022-05-25T14:30:26.221492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>So once again we see that the base XGBoost seems to be performng better w.r.t prediction of positive class and also has better accuracy.</strong>","metadata":{}},{"cell_type":"markdown","source":"#### ROC Curves, Area under ROC Curves","metadata":{}},{"cell_type":"code","source":"probs_y=best_xgb_random.predict_proba(X_test) \nprecision, recall, thresholds = precision_recall_curve(Y_test, probs_y[:, 1]) \nprobs_y_ov=best_xgb_random_ov.predict_proba(X_test) \nprecision_ov, recall_ov, thresholds_ov = precision_recall_curve(Y_test, probs_y_ov[:, 1]) \n\n\nfpr, tpr, thresholds = roc_curve(Y_test, probs_y[:,1], drop_intermediate=False)\nroc_auc = auc(fpr, tpr)\nfpr_ov, tpr_ov, thresholds_ov = roc_curve(Y_test, probs_y_ov[:,1], drop_intermediate=False)\nroc_auc_ov = auc(fpr_ov, tpr_ov)\n\nfig, ax = plt.subplots(1,2, figsize=(16,4))\n\nplt.suptitle('Receiver Operating Characteristics')\n\nax[0].plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nax[0].legend(loc = 'lower right')\nax[0].plot([0, 1], [0, 1],'r--')\nax[0].set_xlim([0, 1])\nax[0].set_ylim([0, 1])\nax[0].set_ylabel('True Positive Rate')\nax[0].set_xlabel('False Positive Rate')\nax[0].set_title(\"XgBoost\")\n\nax[1].plot(fpr_ov, tpr_ov, 'g', label = 'AUC = %0.2f' % roc_auc_ov)\nax[1].legend(loc = 'lower right')\nax[1].plot([0, 1], [0, 1],'r--')\nax[1].set_xlim([0, 1])\nax[1].set_ylim([0, 1])\nax[1].set_ylabel('True Positive Rate')\nax[1].set_xlabel('False Positive Rate')\nax[1].set_title(\"XgBoost with oversampling\")\n\nplt.plot()","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-25T14:30:26.223875Z","iopub.execute_input":"2022-05-25T14:30:26.224653Z","iopub.status.idle":"2022-05-25T14:30:26.577924Z","shell.execute_reply.started":"2022-05-25T14:30:26.224596Z","shell.execute_reply":"2022-05-25T14:30:26.576919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong> So the AUC of the original XGBoost model is better than the AUC of the model trained on oversampled data.</strong>\n\n<strong>So from Section 2, the best model seems to be : <mark>best_xgb_random</mark> </strong>\n#### ","metadata":{}},{"cell_type":"markdown","source":"<a id = \"ten\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Random Forest Classifier ◽\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"#### \n### 3. Random Forest ","metadata":{}},{"cell_type":"markdown","source":" - We will again use 5 fold Cross Validation.\n - We will use Stratified K-Fold.\n - We will use GridSearch to tune the hyperparameters.\n - We will treat class imbalance by treating class weights as hyperparameters.\n - We will consider maximum tree depth, row sampling and number of parallel trees as the other hyperparameters.","metadata":{}},{"cell_type":"code","source":"\n\nclass_wts = [{ 0:1, 1:scale_pos_w },'balanced']\n#hyper parameters for grid search\nparams = {\n        'class_weight': class_wts,\n        'max_samples': [0.6, 0.8],\n        'max_depth': [5, 10, 15],\n        'n_estimators':[300, 500]\n        }\n\n#RF classifier\nrf = RandomForestClassifier(random_state=1001)\n\nfolds = 5\n#Stratified k-folds\nskf = StratifiedKFold(n_splits=folds, shuffle=True, random_state = 1001)\n\n#model with grid search for hyperparameter tuning\ngrid_search_rf = GridSearchCV(rf, params, scoring='f1', n_jobs=4, cv=skf.split(X_train,Y_train))\n\nstart = dt.datetime.now()\ngrid_search_rf.fit(X_train, Y_train)\nend = dt.datetime.now()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:30:26.578903Z","iopub.execute_input":"2022-05-25T14:30:26.579842Z","iopub.status.idle":"2022-05-25T14:31:18.746367Z","shell.execute_reply.started":"2022-05-25T14:30:26.579789Z","shell.execute_reply":"2022-05-25T14:31:18.745363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Best Random Forest Model obtained using Grid Search","metadata":{}},{"cell_type":"code","source":"print('\\n Best hyperparameters:')\nprint(grid_search_rf.best_params_)\ngrid_best_rf = grid_search_rf.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:31:18.747876Z","iopub.execute_input":"2022-05-25T14:31:18.748171Z","iopub.status.idle":"2022-05-25T14:31:18.753876Z","shell.execute_reply.started":"2022-05-25T14:31:18.748137Z","shell.execute_reply":"2022-05-25T14:31:18.752903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_samples = grid_best_rf['max_samples']\nmax_depth = grid_best_rf['max_depth']\nn_estimators = grid_best_rf['n_estimators']\nclass_weight = grid_best_rf['class_weight']\n\nbest_rf = RandomForestClassifier(random_state=1001, max_samples=max_samples, max_depth=max_depth, \n                                n_estimators=n_estimators, class_weight=class_weight)\nbest_rf.fit(X_train, Y_train)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:31:18.755139Z","iopub.execute_input":"2022-05-25T14:31:18.755363Z","iopub.status.idle":"2022-05-25T14:31:19.706793Z","shell.execute_reply.started":"2022-05-25T14:31:18.755337Z","shell.execute_reply":"2022-05-25T14:31:19.705896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Time taken for training : {end - start}\\nTraining F1 score:{best_rf.score(X_train, Y_train)}\\nTest F1 score: {best_rf.score(X_test, Y_test)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:31:19.708173Z","iopub.execute_input":"2022-05-25T14:31:19.708472Z","iopub.status.idle":"2022-05-25T14:31:19.896923Z","shell.execute_reply.started":"2022-05-25T14:31:19.708431Z","shell.execute_reply":"2022-05-25T14:31:19.895923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>So our random forest model seems to be performing worse than the XGBoost model in terms of the F1 score of the attrited class, on test data. </strong>","metadata":{}},{"cell_type":"markdown","source":"<a id = \"eleven\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Comparison of Different Models ◽\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"#### \n\n### Comparing the simple decision tree, random forest and gradient boosted decision trees models.","metadata":{}},{"cell_type":"markdown","source":"#### 1. Feature Importances","metadata":{}},{"cell_type":"code","source":"dt_feat_imp = model1.feature_importances_\nxgb_feat_imp = best_xgb_random.feature_importances_\nrf_feat_imp = best_rf.feature_importances_\n\ndf_dt = pd.DataFrame()\ndf_dt['data'] = dt_feat_imp\ndf_dt['mdl'] = ['Decision Tree' for _ in range(len(dt_feat_imp))]\ndf_xgb = pd.DataFrame()\ndf_xgb['data'] = xgb_feat_imp\ndf_xgb['mdl'] = ['XGBoost' for _ in range(len(xgb_feat_imp))]\ndf_rf = pd.DataFrame()\ndf_rf['data'] = rf_feat_imp\ndf_rf['mdl'] = ['Random Forest' for _ in range(len(rf_feat_imp))]\nfinal = df_dt.append(df_xgb.append(df_rf))\nfinal[\"col\"] = list(X_train.columns)*3\n\nplt.figure(figsize=(12,12))\nsns.barplot(y=final['col'], x=final['data'], hue=final['mdl'], orient='h')\nplt.xlabel(\"Feature Importance\")\nplt.ylabel(\"Feature\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:31:19.898633Z","iopub.execute_input":"2022-05-25T14:31:19.899309Z","iopub.status.idle":"2022-05-25T14:31:20.397227Z","shell.execute_reply.started":"2022-05-25T14:31:19.899258Z","shell.execute_reply":"2022-05-25T14:31:20.396177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- We can see that the feature 'Quarterly Rating' seems to be very important for the Decision Tree and XGBoost models.\n\n- For Random Forest, Experience seems to be the most important feature, followed by Quarterly Rating. Experience is also important for Decision Tree Model. But it's importance is quite low for the XGBoost model. We observe something similar for the Total Business Value feature.\n\n- The features Age and Income are important for Random Forest, but not so much for Decision Tree or XGBoost.","metadata":{}},{"cell_type":"markdown","source":"#### \n#### 2. Precision, Recall and F1 scores","metadata":{}},{"cell_type":"code","source":"Y_test_pred_dt = model1.predict(X_test)\nY_test_pred_xgb = best_xgb_random.predict(X_test)\nY_test_pred_rf = best_rf.predict(X_test)\n\nprint(\"Decision Tree model:\")\nprint(classification_report(Y_test, Y_test_pred_dt))\nprint(\"-\"*60)\nprint(\"\\nXgBoost model:\")\nprint(classification_report(Y_test, Y_test_pred_xgb))\nprint(\"-\"*60)\nprint(\"\\nRandom Forest:\")\nprint(classification_report(Y_test, Y_test_pred_rf))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:31:20.398817Z","iopub.execute_input":"2022-05-25T14:31:20.399351Z","iopub.status.idle":"2022-05-25T14:31:20.498716Z","shell.execute_reply.started":"2022-05-25T14:31:20.399299Z","shell.execute_reply":"2022-05-25T14:31:20.497854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>The <mark>XGBoost</mark> model has the highest <mark>precision</mark>. It's recall is one percent lower than the Decision Tree Model. However, we get the best F1 score for the XGBoost model. </strong>\n- <strong>The <mark>F1 score</mark> for <mark>XGBoost model</mark> is <mark>0.87</mark>, while for the other two models it is 0.86.</strong>\n\n- <strong>If we look at <mark>weighted F1 score</mark>, then <mark>XGBoost</mark> appears to be the clear winner with a score of <mark>0.82</mark>, whereas the score for the Decision Tree model is 0.8 and for the Random Forest model, is 0.81.</strong>","metadata":{}},{"cell_type":"markdown","source":"#### \n#### 3. Accuracy and Confusion Matrices","metadata":{}},{"cell_type":"code","source":"print(\"Test accuracy for Decision Tree -\", accuracy_score(Y_test, Y_test_pred_dt))\nprint(\"Test accuracy for XGBoost -\", accuracy_score(Y_test, Y_test_pred_xgb))\nprint(\"Test accuracy for Random Forest -\", accuracy_score(Y_test, Y_test_pred_rf))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:31:20.500053Z","iopub.execute_input":"2022-05-25T14:31:20.500876Z","iopub.status.idle":"2022-05-25T14:31:20.509869Z","shell.execute_reply.started":"2022-05-25T14:31:20.500812Z","shell.execute_reply":"2022-05-25T14:31:20.50893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Confusion matrix format:\")\nprint(\"   0   1\")\nprint(\"   _____\")\nprint(\"0 |__|__|\")\nprint(\"1 |__|__|\")\nprint(\"-\"*60)\n\nprint(\"\\nDecision Tree:\")\nprint(confusion_matrix(Y_test, Y_test_pred_dt))\nprint(\"-\"*60)\nprint(\"\\nXGBoost model:\")\nprint(confusion_matrix(Y_test, Y_test_pred_xgb))\nprint(\"-\"*60)\nprint(\"\\nRandom Forest:\")\nprint(confusion_matrix(Y_test, Y_test_pred_rf))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2022-05-25T14:31:20.511496Z","iopub.execute_input":"2022-05-25T14:31:20.511726Z","iopub.status.idle":"2022-05-25T14:31:20.528622Z","shell.execute_reply.started":"2022-05-25T14:31:20.511699Z","shell.execute_reply":"2022-05-25T14:31:20.528013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- <strong>In terms of accuracy again, the <mark>XGBoost</mark> model has highest score of <mark>0.82</mark>, while the other models have around 0.80.</strong>\n\n- <strong>If we look at the confusion matrix, again the <mark>XGBoost</mark> model seems to be performing <mark>reasonably well</mark> in terms of reducing misclassifications of both the positive and negative classes. </strong>\n\n- <strong>Decision tree achieves only marginally better performance for positive class, but misclassifies a lot of negative class observations. Random forest does better on negative class than decision tree, but make it worse for the positive class. So the <mark>XGBoost</mark> model provides a balance between the two. </strong>","metadata":{}},{"cell_type":"markdown","source":"#### \n#### 4. ROC Curves, Area under ROC Curves","metadata":{}},{"cell_type":"code","source":"probs_y_dt=model1.predict_proba(X_test) \nprecision_dt, recall_dt, thresholds_dt = precision_recall_curve(Y_test, probs_y_dt[:, 1]) \nprobs_y_xgb=best_xgb_random.predict_proba(X_test) \nprecision_xgb, recall_xgb, thresholds_xgb = precision_recall_curve(Y_test, probs_y_xgb[:, 1]) \nprobs_y_rf = best_rf.predict_proba(X_test)\nprecision_rf, recall_rf, thresholds_rf = precision_recall_curve(Y_test, probs_y_rf[:, 1]) \n\n\nfpr_dt, tpr_dt, thresholds_dt = roc_curve(Y_test, probs_y_dt[:,1], drop_intermediate=False)\nroc_auc_dt = auc(fpr_dt, tpr_dt)\nfpr_xgb, tpr_xgb, thresholds_xgb = roc_curve(Y_test, probs_y_xgb[:,1], drop_intermediate=False)\nroc_auc_xgb = auc(fpr_xgb, tpr_xgb)\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(Y_test, probs_y_rf[:,1], drop_intermediate=False)\nroc_auc_rf = auc(fpr_rf, tpr_rf)\n\nfig, ax = plt.subplots(1,3, figsize=(18,4))\n\nplt.suptitle('Receiver Operating Characteristics')\n\nax[0].plot(fpr_dt, tpr_dt, 'b', label = 'AUC = %0.2f' % roc_auc_dt)\nax[0].legend(loc = 'lower right')\nax[0].plot([0, 1], [0, 1],'r--')\nax[0].set_xlim([0, 1])\nax[0].set_ylim([0, 1])\nax[0].set_ylabel('True Positive Rate')\nax[0].set_xlabel('False Positive Rate')\nax[0].set_title(\"Decision Tree\")\n\nax[1].plot(fpr_xgb, tpr_xgb, 'g', label = 'AUC = %0.2f' % roc_auc_xgb)\nax[1].legend(loc = 'lower right')\nax[1].plot([0, 1], [0, 1],'r--')\nax[1].set_xlim([0, 1])\nax[1].set_ylim([0, 1])\nax[1].set_ylabel('True Positive Rate')\nax[1].set_xlabel('False Positive Rate')\nax[1].set_title(\"XgBoost\")\n\nax[2].plot(fpr_rf, tpr_rf, 'orange', label = 'AUC = %0.2f' % roc_auc_rf)\nax[2].legend(loc = 'lower right')\nax[2].plot([0, 1], [0, 1],'r--')\nax[2].set_xlim([0, 1])\nax[2].set_ylim([0, 1])\nax[2].set_ylabel('True Positive Rate')\nax[2].set_xlabel('False Positive Rate')\nax[2].set_title(\"Random Forest\")\n\nplt.plot()","metadata":{"execution":{"iopub.status.busy":"2022-05-25T14:31:20.530655Z","iopub.execute_input":"2022-05-25T14:31:20.531205Z","iopub.status.idle":"2022-05-25T14:31:21.05081Z","shell.execute_reply.started":"2022-05-25T14:31:20.531156Z","shell.execute_reply":"2022-05-25T14:31:21.049461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### **<span style=\"color:#483D8B;\">Observation: </span>**\n- The AUC for both XGBoost and Random Forests seems to be same (0.88) while for the decision tree it is 0.82.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"twelve\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Insights obtained using Exploratory Data Analysis ◽\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"#### \n### Insights from EDA\n\n- Almost 67.8 % of the employees have left the organization. \n- The median age of the employees is 33. There are employees as young as 21 years and as old as 58 years.\n- The median income is about 55K. It can range from 10K all the way to 188K.\n- The median business value generated is 817K.\n- The median experience is between 5 to 6 months. \n- We see that there are 59% male employees and 41% female employees.\n- The percentages of employees with different education levels are almost equal (~33%).\n- 98.2% of the employees did not receive any income increment. Only 1.8% received a raise.\n- Almost 43% of the employees joined at lowest designation (1). 34% joined at level 2, 20% at level 3 and below 2% joined at higher levels.\n- Majority (35%) of the employees currently are at designation level 2, followed by designation level 1 (31%) and 3 (26%). Less than 6% of the employees are currently in higher designations.\n- Only 17% of the employees received a promotion, while 83% did not. However, we saw that only 1.8% received a raise in income. So it seems like these people are getting ahead in their grade without receiving the financial compensation. This might lead to dissatisfaction. \n- Quarterly Rating is lowest (1) for the majority of employees (73%). Very few received rating over 3 (11.5%).\n- Quarterly rating increased for only 15% of employees. It reduced for 19% of the employees and remained unchanged for 65%.\n- The majority of the employees seem to be associated with city C20.\n- The median age for attrited employees is 33, while for non attrited employees it is 34.\n- The median income of atrited employyes is lower (\\~51K) than non attrited employees (\\~64K).\n- The median total business value generated by attrited employees (\\~46K) is almost 1/6 of the non attrited employees (\\~263K)\n- The median experience for attrited employees is about 165 days and for non attrited employees it is 186 days.\n- The attrition rate of male and female employees is similar (\\~67-68%).\n- The attrition rate across different education levels is also similar (from 66-69%).\n- The attrition among those who received raise (7%) is far lower than those who did not receive raise (68%).\n- The atrition is highest among those whose current designation is 1 (80%) followed by 2 (70%). The rate is around 50-54% for the remaining designation.\n- The attrition rate is 70% for those who were not promoted and 54% for those who were promoted.\n- The attrtion rate follows a clear declinig pattern with increasing quarterly rating : 1 (82%), 2 (40%), 3 (16%) and 4 (9%).\n- The attrtion rate is highest among those whose quarterly rating decreased (81%), followed by those whose rating remained unchanged (74%) and finally those whose rating increased (23%).\n- There is mopderate correlation between the Total Business Value and Experience features, which is somewhat expected, since the longer a person works, the more likely it is for him to generate positive business value and higher the sum of monthly business values.\n- There is moderate correlation between Current Designation and Income which is expected since the higher the designation, the greater the income.\n- Also, there is moderate correlation between the current designation and joining designation.\n- Again, there is moderate correlation between Experience and Promotion (the longer a person has been working, the higher the chance of getting promoted.","metadata":{}},{"cell_type":"markdown","source":"<a id = \"thirteen\"></a>\n<h1 id=\"basics\" style=\"background:black; padding: 10px 0; border:#FFC300; border-width:5px; border-radius: 10px; border-style:solid; font-family:MV Boli; color:white; line-height: 10px;\"> \n    <center> ◽ Discussion on Trade Off and Possible Recommendations ◽\n    </center>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"#### \n#### The Trade-Off\n\nIn general while choosing a model, we might choose to look at precision and recall scores and choose while keeping the following trade-off on mind :\n- If we prioritize precision, we are going to reduce our false positives. This may be useful if our targeted retention strategies prove to be expensive. We don't want to spend unnecessarily on somebody who is not even going to leave in the first place. Also, it might lead to uncomfortable situation for the employee themselves if they are put in a situation where it is assumed that they are going to be let go/ going to leave.\n\n- If we prioritize recall, we are going to reduce our false negatives. This is useful since usually the cost of hiring a new person is higher than retaining an experienced person. So, by reducing false negatives, we would be able to better identify those who are actually going to leave and try to retain them by appropriate measures (competitive remuneration, engagement program, etc). ","metadata":{}},{"cell_type":"markdown","source":"### Recommendations \n\n- We saw that the percentage of employees who received a raise was only 1.8%, whereas 17% of the employees received a promotion. It appears that promotion does not guarantee an increase in compensation. Employees may not be motivated to continue working for an organization if they feel that they are undervalued in terms of financial compensation. This is further substantiated by the fact that attrition rate among promoted employees was 54%, which was a lot higher than the attrition rate of employees who received a raise (7%). So it is better if promotion also leads to increase in compensation.\n\n- We saw that quarterly rating is an important indicator of whether the employee might leave voluntarily/involuntarily. A lot of employees have the lowest quarterly rating and the attrition among them is high. The organization might look into improvement programs for underperforming employees before they are let go, or before they resign. Also, improvement in rating can be rewarded to encourage better performance. \n\n- Salaries must be competitve to prevent losing employees to attrition and to competition and since almost 68% of employees attrited in a small span, it seems that correction of payscales is necessary for the organization since hiring new employees is costlier than retaining old ones.\n\n- Specific employee engagement programs can also be introduced targeting the different categories of performers to help them achieve their goals as well as help them increase their contribution towards the business value. The hiring process can also be restructured since a lot of new hires seem to be leaving (median experience is lower for attrited employees).","metadata":{}},{"cell_type":"markdown","source":"### Thank You for Reading! Please add your suggestions/comments and upvote if you liked my work!","metadata":{}}]}